{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Uk Parliament Debate Files - infformation retrieval and processing\n",
    "\n",
    "### Matan  Aminoff 326636545, Noam Solow - 214928517"
   ],
   "id": "613fbedde1c4c0fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section A: Download Debate FilesÂ§",
   "id": "8811318b2272b8af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to https://www.theyworkforyou.com/pwdata/scrapedxml/debates/ to get the file list...\n",
      "Found 19692 total XML files on the server.\n",
      "Found start file. Preparing to download 956 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 956/956 [15:51<00:00,  1.00file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete!\n",
      "All files are saved in the 'xml_files' directory.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1,
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_debate_files():\n",
    "    \"\"\"\n",
    "    Downloads UK Parliament debate files from the specified start file.\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.theyworkforyou.com/pwdata/scrapedxml/debates/\"  # \n",
    "    start_file = \"debates2023-06-28d.xml\"  # \n",
    "    output_dir = \"data.xml_files\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    print(f\"Connecting to {base_url} to get the file list...\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()  \n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        all_xml_files = []\n",
    "        for link in soup.find_all('a'):\n",
    "            href = link.get('href')\n",
    "            if href and href.endswith('.xml'):\n",
    "                all_xml_files.append(href)\n",
    "        \n",
    "        print(f\"Found {len(all_xml_files)} total XML files on the server.\")\n",
    "\n",
    "        try:\n",
    "            start_index = all_xml_files.index(start_file)\n",
    "            files_to_download = all_xml_files[start_index:]\n",
    "            print(f\"Found start file. Preparing to download {len(files_to_download)} files...\")\n",
    "        except ValueError:\n",
    "            print(f\"Error: Could not find the starting file '{start_file}' in the list.\")\n",
    "            print(\"Please check the file name and try again.\")\n",
    "            return\n",
    "\n",
    "      \n",
    "        for filename in tqdm(files_to_download, desc=\"Downloading files\", unit=\"file\"):\n",
    "            file_url = urljoin(base_url, filename)\n",
    "            local_path = os.path.join(output_dir, filename)\n",
    "\n",
    "            if os.path.exists(local_path):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                file_response = requests.get(file_url)\n",
    "                file_response.raise_for_status()\n",
    "                \n",
    "                with open(local_path, 'wb') as f:\n",
    "                    f.write(file_response.content)\n",
    "            \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Failed to download {filename}: {e}\")\n",
    "\n",
    "        print(\"\\nDownload complete!\")\n",
    "        print(f\"All files are saved in the '{output_dir}' directory.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to access the website {base_url}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_debate_files()"
   ],
   "id": "ef73fae689851848"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section B: Extract and Combine Text from Debate Files\n",
    "\n",
    "So far we have downloaded the raw XML files. Now we will extract the text from them and combine them into single text files per date.\n",
    "We will focus on extracting text from `<p>` tags only, ignoring redirect files.\n"
   ],
   "id": "ac107d1cf5f4c2e5"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c5ad86daea14141",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T13:03:25.690927Z",
     "start_time": "2025-11-12T13:03:18.173509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "× ××¦××• 956 ×§×‘×¦×™×. ××ª×—×™×œ×™× ×‘×—×™×œ×•×¥ ×•××™×—×•×“...\n",
      "×¡×™×•× ×—×™×œ×•×¥. × ××¦××• 336 ×ª××¨×™×›×™× ×™×™×—×•×“×™×™×.\n",
      "×”×˜×§×¡×˜×™× ×”×××•×—×“×™× (××‘×•×¡×¡×™ <p>) × ×©××¨×• ×‘×”×¦×œ×—×” ×‘×¡×¤×¨×™×™×” **data\\combined_xml_files**\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "XML_DIR = os.path.join('data', 'xml_files')\n",
    "OUTPUT_DIR = os.path.join('data', 'combined_xml_files')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def extract_text_from_xml(xml_file_path):\n",
    "    all_text = []\n",
    "    \n",
    "    try:\n",
    "        tree = ET.parse(xml_file_path)\n",
    "        root = tree.getroot()\n",
    "\n",
    "       # basically extract all <p> tags text\n",
    "\n",
    "        for p_tag in root.findall('.//p'):\n",
    "            if p_tag.text and p_tag.text.strip():\n",
    "                all_text.append(p_tag.text.strip())\n",
    "\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"×©×’×™××ª × ×™×ª×•×— XML ×‘×§×•×‘×¥ {xml_file_path}: {e}\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"×©×’×™××” ×›×œ×œ×™×ª ×‘×§×•×‘×¥ {xml_file_path}: {e}\")\n",
    "        return \"\"\n",
    "        \n",
    "    return ' '.join(all_text)\n",
    "\n",
    "# extract and combine texts\n",
    "combined_texts = defaultdict(list)\n",
    "file_list = os.listdir(XML_DIR)\n",
    "\n",
    "print(f\"× ××¦××• {len(file_list)} ×§×‘×¦×™×. ××ª×—×™×œ×™× ×‘×—×™×œ×•×¥ ×•××™×—×•×“...\")\n",
    "\n",
    "for filename in file_list:\n",
    "    if not filename.endswith('.xml'):\n",
    "        continue\n",
    "        \n",
    "    file_path = os.path.join(XML_DIR, filename)\n",
    "    \n",
    "    match = re.search(r'debates(\\d{4}-\\d{2}-\\d{2})[a-zA-Z]*d?\\.xml$', filename)\n",
    "    if not match:\n",
    "        continue\n",
    "        \n",
    "    base_date = match.group(1)\n",
    "    \n",
    "    raw_text = extract_text_from_xml(file_path)\n",
    "    \n",
    "    if raw_text:\n",
    "        combined_texts[base_date].append(raw_text)\n",
    "\n",
    "print(f\"×¡×™×•× ×—×™×œ×•×¥. × ××¦××• {len(combined_texts)} ×ª××¨×™×›×™× ×™×™×—×•×“×™×™×.\")\n",
    "\n",
    "# ×©××™×¨×ª ×”×§×‘×¦×™× ×”×××•×—×“×™×\n",
    "for date, text_list in combined_texts.items():\n",
    "    final_combined_text = ' '.join(text_list)\n",
    "    \n",
    "    # ×©× ×”×§×•×‘×¥ ×”×××•×—×“: debates_YYYY-MM-DD.txt\n",
    "    output_filename = f\"debates_{date}.txt\"\n",
    "    output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(final_combined_text)\n",
    "\n",
    "print(f\"×”×˜×§×¡×˜×™× ×”×××•×—×“×™× (××‘×•×¡×¡×™ <p>) × ×©××¨×• ×‘×”×¦×œ×—×” ×‘×¡×¤×¨×™×™×” **{OUTPUT_DIR}**\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section C1: Punctuation spacing using NLTK\n",
    "in this section we will use NLTK's word_tokenize to properly space punctuation in the text files.\n",
    "the nltk tokenizer is more sophisticated than simple regex and handles edge cases better,\n",
    "though it may be less accurate than spaCy for complex texts."
   ],
   "id": "6f3c10221cb7b0ca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#\n",
    "INPUT_DIR = os.path.join('data', 'combined_xml_files')\n",
    "OUTPUT_DIR = os.path.join('data', 'tokenized_text_nltk') \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"××ª×—×™×œ × ×™×§×•×™ ×¡×™×× ×™ ×¤×™×¡×•×§ (×¢× NLTK). ×§×‘×¦×™× ×™×™×©××¨×• ×‘- {OUTPUT_DIR}\")\n",
    "\n",
    "#\n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_path = os.path.join(INPUT_DIR, filename)\n",
    "        \n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "            \n",
    "        #tokenize the text by NLTK\n",
    "        tokens = word_tokenize(raw_text)\n",
    "        \n",
    "        # join tokens back to string with spaces\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        output_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_text)\n",
    "\n",
    "print(\"×¡×™×•× × ×™×§×•×™ ×¡×™×× ×™ ×¤×™×¡×•×§ (NLTK).\")"
   ],
   "id": "9e6c3b172efad03a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section C2: Punctuation spacing using spaCy \n",
    "This section uses spaCy for tokenization, which is generally more accurate and robust than NLTK's tokenizer."
   ],
   "id": "2b342f347ff1da3b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eddbe8ad27931695",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T14:24:40.181340Z",
     "start_time": "2025-11-12T13:28:49.312642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "×˜×•×¢×Ÿ ××ª ××•×“×œ ×”×©×¤×” ×©×œ spaCy...\n",
      "×”××•×“×œ × ×˜×¢×Ÿ.\n",
      "××ª×—×™×œ × ×™×§×•×™ ×¡×™×× ×™ ×¤×™×¡×•×§ (×¢× spaCy). ×§×‘×¦×™× ×™×™×©××¨×• ×‘- data\\tokenized_text_spacy\n",
      "×¡×™×•× × ×™×§×•×™ ×¡×™×× ×™ ×¤×™×¡×•×§ (spaCy).\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import os\n",
    "\n",
    "INPUT_DIR = os.path.join('data', 'combined_xml_files')\n",
    "OUTPUT_DIR = os.path.join('data', 'tokenized_text_spacy') # ×©× ×—×“×©\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"×˜×•×¢×Ÿ ××ª ××•×“×œ ×”×©×¤×” ×©×œ spaCy...\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\"×”××•×“×œ × ×˜×¢×Ÿ.\")\n",
    "\n",
    "print(f\"××ª×—×™×œ × ×™×§×•×™ ×¡×™×× ×™ ×¤×™×¡×•×§ (×¢× spaCy). ×§×‘×¦×™× ×™×™×©××¨×• ×‘- {OUTPUT_DIR}\")\n",
    "\n",
    "# ×§×¨×™××” ×©×œ ×›×œ ×”×§×‘×¦×™× ×”×××•×—×“×™×\n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_path = os.path.join(INPUT_DIR, filename)\n",
    "        \n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            raw_text = f.read()\n",
    "            \n",
    "        \n",
    "        doc = nlp(raw_text) # Spacy tokenization\n",
    "        \n",
    "        #for each token get the text\n",
    "        tokens = [token.text for token in doc]\n",
    "        \n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        \n",
    "        output_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        \n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(cleaned_text)\n",
    "\n",
    "print(\"×¡×™×•× × ×™×§×•×™ ×¡×™×× ×™ ×¤×™×¡×•×§ (spaCy).\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section D: Lemmatization using spaCy\n",
    "So until now we have tokenized text files. In this section we will perform lemmatization using spaCy."
   ],
   "id": "652bfce828a7e5e5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import spacy\n",
    "import os\n",
    "\n",
    "INPUT_DIR = os.path.join('data', 'tokenized_text_spacy') \n",
    "OUTPUT_DIR = os.path.join('data', 'lemmatized_text')    \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print(\"×˜×•×¢×Ÿ ××ª ××•×“×œ ×”×©×¤×” ×©×œ spaCy (×¢× ×œ××˜×™×–×¦×™×”)...\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
    "print(\"×”××•×“×œ × ×˜×¢×Ÿ.\")\n",
    "\n",
    "# 2. ×”×›× ×ª ×¨×©×™××ª ×”×§×‘×¦×™× ×œ×¢×™×‘×•×“\n",
    "file_paths = []\n",
    "for filename in os.listdir(INPUT_DIR):\n",
    "    if filename.endswith('.txt'):\n",
    "        input_path = os.path.join(INPUT_DIR, filename)\n",
    "        output_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        file_paths.append((input_path, output_path))\n",
    "\n",
    "# 3. ×™×¦×™×¨×ª \"×’× ×¨×˜×•×¨\" ×©×§×•×¨× ××ª ×”×§×‘×¦×™×\n",
    "def text_generator(paths):\n",
    "    for in_path, _ in paths:\n",
    "        with open(in_path, 'r', encoding='utf-8') as f:\n",
    "            yield f.read()\n",
    "\n",
    "print(f\"××ª×—×™×œ ×¢×™×‘×•×“ ××”×™×¨ (batch) ×¢× spaCy... ××‘×¦×¢ ×œ××˜×™×–×¦×™×” ×¢×œ {len(file_paths)} ×§×‘×¦×™×.\")\n",
    "\n",
    "# 4. ×¢×™×‘×•×“ ×›×œ ×”×˜×§×¡×˜×™× ×‘-BATCH\n",
    "processed_docs = nlp.pipe(text_generator(file_paths), batch_size=50)\n",
    "\n",
    "# 5. ×©××™×¨×ª ×”×ª×•×¦××•×ª\n",
    "for (in_path, out_path), doc in zip(file_paths, processed_docs):\n",
    "    \n",
    "    # ×—×™×œ×•×¥ ×”×œ××” (lemma_) ×©×œ ×›×œ ×˜×•×§×Ÿ\n",
    "    # ×× ×• ×’× ×××™×¨×™× ×”×›×œ ×œ××•×ª×™×•×ª ×§×˜× ×•×ª (lowercase)\n",
    "    lemmas = [token.lemma_.lower() for token in doc]\n",
    "    lemmatized_text = ' '.join(lemmas)\n",
    "    \n",
    "    # ×©××™×¨×ª ×”×§×•×‘×¥ ×”× ×§×™\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(lemmatized_text)\n",
    "\n",
    "print(f\"×¡×™×•× ×¢×™×‘×•×“ (Lemmatization). ×”×§×‘×¦×™× × ×©××¨×• ×‘- {OUTPUT_DIR}\")"
   ],
   "id": "83a09c177f707f61"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section E: BM25 Vectorization without Optimizatios\n",
    "right now we will create BM25 vectors for each document and save them as JSON files.\n",
    "first we will filter out rare words that appear less than 5 times across all documents.\n",
    "second we will compute the BM25 scores for each term in each document and save the non-zero scores in a JSON file.\n",
    "\n",
    "This is the basic version without optimizations.\n",
    "It computes BM25 scores for all terms in the vocabulary for each document,\n"
   ],
   "id": "fd6911a74b6486cc"
  },
  {
   "cell_type": "code",
   "id": "b9157389672487c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T08:06:30.187293Z",
     "start_time": "2025-11-19T08:06:27.822764Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\"\"\"\n",
    "load the texts from a folder, tokenize and clean them\n",
    "\"\"\"\n",
    "def load_texts(folder_path):\n",
    "    docs, filenames = [], []\n",
    "    for file_path in glob.glob(os.path.join(folder_path, \"*.txt\")):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().lower()\n",
    "            tokens = [w for w in word_tokenize(text) if w.isalpha() and w not in STOP_WORDS]# remove punctuation and stop words\n",
    "            docs.append(tokens) #\n",
    "            filenames.append(os.path.basename(file_path))\n",
    "    return docs, filenames\n",
    "\n",
    "\"\"\"\n",
    "filter out rare words that appear less than min_freq times across all documents\n",
    "\"\"\"\n",
    "def filter_rare_words(docs, min_freq=5):\n",
    "    \n",
    "    freq = Counter([word for doc in docs for word in doc])\n",
    "    filtered_docs = [[w for w in doc if freq[w] >= min_freq] for doc in docs]\n",
    "    return filtered_docs\n",
    "\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "def save_json_vectors(docs, filenames, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    bm25 = BM25Okapi(docs)\n",
    "    #\n",
    "    for doc_tokens, fname in zip(docs, filenames):\n",
    "        vector = {}\n",
    "        for term in bm25.idf.keys():\n",
    "            f = doc_tokens.count(term)\n",
    "            #\n",
    "            if f > 0:\n",
    "                score = bm25.idf[term] * ((f * (bm25.k1 + 1)) / (f + bm25.k1 * (1 - bm25.b + bm25.b * len(doc_tokens) / bm25.avgdl)))\n",
    "                vector[term] = round(score, 3)\n",
    "        with open(os.path.join(output_dir, fname.replace(\".txt\", \".json\")), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(vector, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "base_data = \"data\"\n",
    "base_model = \"models\"\n",
    "\n",
    "# ×¢×™×‘×•×“ ×’×¨×¡×ª spaCy\n",
    "spacy_docs, spacy_names = load_texts(os.path.join(base_data, \"tokenized_text_spacy\"))\n",
    "spacy_docs = filter_rare_words(spacy_docs)\n",
    "save_json_vectors(spacy_docs, spacy_names, os.path.join(base_model, \"bm25_word_json_dict\"))\n",
    "\n",
    "# ×¢×™×‘×•×“ ×’×¨×¡×ª Lemmatized\n",
    "lemm_docs, lemm_names = load_texts(os.path.join(base_data, \"lemmatized_text\"))\n",
    "lemm_docs = filter_rare_words(lemm_docs)\n",
    "save_json_vectors(lemm_docs, lemm_names, os.path.join(base_model, \"bm25_lemm_json_dict\"))\n",
    "\n",
    "print(\"âœ… Done! All vectors saved per document.\")\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/matanaminov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/matanaminov/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[65], line 58\u001B[0m\n\u001B[1;32m     55\u001B[0m base_model \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# ×¢×™×‘×•×“ ×’×¨×¡×ª spaCy\u001B[39;00m\n\u001B[0;32m---> 58\u001B[0m spacy_docs, spacy_names \u001B[38;5;241m=\u001B[39m \u001B[43mload_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_data\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokenized_text_spacy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m spacy_docs \u001B[38;5;241m=\u001B[39m filter_rare_words(spacy_docs)\n\u001B[1;32m     60\u001B[0m save_json_vectors(spacy_docs, spacy_names, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(base_model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbm25_word_json_dict\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
      "Cell \u001B[0;32mIn[65], line 23\u001B[0m, in \u001B[0;36mload_texts\u001B[0;34m(folder_path)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(file_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m     22\u001B[0m     text \u001B[38;5;241m=\u001B[39m f\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39mlower()\n\u001B[0;32m---> 23\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m [w \u001B[38;5;28;01mfor\u001B[39;00m w \u001B[38;5;129;01min\u001B[39;00m \u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m w\u001B[38;5;241m.\u001B[39misalpha() \u001B[38;5;129;01mand\u001B[39;00m w \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m STOP_WORDS]\u001B[38;5;66;03m# remove punctuation and stop words\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     docs\u001B[38;5;241m.\u001B[39mappend(tokens) \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     filenames\u001B[38;5;241m.\u001B[39mappend(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mbasename(file_path))\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ir_project/lib/python3.10/site-packages/nltk/tokenize/__init__.py:143\u001B[0m, in \u001B[0;36mword_tokenize\u001B[0;34m(text, language, preserve_line)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;124;03mReturn a tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03musing NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;124;03m:type preserve_line: bool\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    142\u001B[0m sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m sent_tokenize(text, language)\n\u001B[0;32m--> 143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m    144\u001B[0m     token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m _treebank_word_tokenizer\u001B[38;5;241m.\u001B[39mtokenize(sent)\n\u001B[1;32m    145\u001B[0m ]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ir_project/lib/python3.10/site-packages/nltk/tokenize/__init__.py:144\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;124;03mReturn a tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03musing NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;124;03m:type preserve_line: bool\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    142\u001B[0m sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m sent_tokenize(text, language)\n\u001B[1;32m    143\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[0;32m--> 144\u001B[0m     token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m \u001B[43m_treebank_word_tokenizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msent\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    145\u001B[0m ]\n",
      "File \u001B[0;32m/opt/anaconda3/envs/ir_project/lib/python3.10/site-packages/nltk/tokenize/destructive.py:158\u001B[0m, in \u001B[0;36mNLTKWordTokenizer.tokenize\u001B[0;34m(self, text, convert_parentheses, return_str)\u001B[0m\n\u001B[1;32m    150\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    151\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParameter \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreturn_str\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m has been deprecated and should no \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    152\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlonger be used.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    153\u001B[0m         category\u001B[38;5;241m=\u001B[39m\u001B[38;5;167;01mDeprecationWarning\u001B[39;00m,\n\u001B[1;32m    154\u001B[0m         stacklevel\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m    155\u001B[0m     )\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m regexp, substitution \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mSTARTING_QUOTES:\n\u001B[0;32m--> 158\u001B[0m     text \u001B[38;5;241m=\u001B[39m \u001B[43mregexp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msub\u001B[49m\u001B[43m(\u001B[49m\u001B[43msubstitution\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    160\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m regexp, substitution \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mPUNCTUATION:\n\u001B[1;32m    161\u001B[0m     text \u001B[38;5;241m=\u001B[39m regexp\u001B[38;5;241m.\u001B[39msub(substitution, text)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section E: BM25 Vectorization with Rare Word Filtering and Optimizations\n",
    "In this section we will optimize the BM25 vectorization process by implementing two key optimizations:\n",
    "1. **Rare Word Filtering**: We will filter out rare words that appear less than a specified minimum frequency across all documents before computing BM25 scores. This reduces the vocabulary size and speeds up computations.\n",
    "2. **Optimized Score Calculation**: Instead of iterating over the entire vocabulary for each document, we will only compute BM25 scores for the words that actually appear in each document. This significantly reduces the number of calculations needed, especially for large vocabularies."
   ],
   "id": "72bd26419356877"
  },
  {
   "cell_type": "code",
   "id": "7577c4620a4de668",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-18T14:53:10.364075Z",
     "start_time": "2025-11-18T14:51:36.435724Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from rank_bm25 import BM25Okapi\n",
    "from collections import Counter\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "def load_texts(folder_path):\n",
    "    docs, filenames = [], []\n",
    "    print(f\"×˜×•×¢×Ÿ ×•×× ×§×” ×˜×§×¡×˜×™× ×: {folder_path}\")\n",
    "    for file_path in glob.glob(os.path.join(folder_path, \"*.txt\")):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read().lower()\n",
    "            tokens = [w for w in word_tokenize(text) if w.isalpha() and w not in STOP_WORDS]\n",
    "            docs.append(tokens)\n",
    "            filenames.append(os.path.basename(file_path))\n",
    "    return docs, filenames\n",
    "\n",
    "def filter_rare_words(docs, min_freq=5):\n",
    "    print(\"××¡× ×Ÿ ××™×œ×™× × ×“×™×¨×•×ª...\")\n",
    "    freq = Counter([word for doc in docs for word in doc])\n",
    "    # ×©×•××¨×™× ××ª ××•×¦×¨ ×”××™×œ×™× ×©××™× ×• × ×“×™×¨\n",
    "    vocab = set(word for word, count in freq.items() if count >= min_freq)\n",
    "    filtered_docs = [[w for w in doc if w in vocab] for doc in docs]\n",
    "    print(f\"×’×•×“×œ ××•×¦×¨ ×”××™×œ×™× ×”××§×•×¨×™: {len(freq)}, ××—×¨×™ ×¡×™× ×•×Ÿ: {len(vocab)}\")\n",
    "    return filtered_docs, vocab\n",
    "\n",
    "def save_json_vectors_optimized(docs, filenames, output_dir, vocab):\n",
    "    \"\"\"\n",
    "    ×’×¨×¡×” ××”×™×¨×”: ××—×©×‘×ª BM25 ×¨×§ ×¢×‘×•×¨ ×”××™×œ×™× ×©×‘×××ª ××•×¤×™×¢×•×ª ×‘×›×œ ××¡××š.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"×‘×•× ×” ××•×“×œ BM25 Okapi ×¢×‘×•×¨ {output_dir}...\")\n",
    "    bm25 = BM25Okapi(docs)\n",
    "    \n",
    "    print(\"×™×•×¦×¨ ×•×©×•××¨ ×•×§×˜×•×¨×™× (×‘×©×™×˜×” ×”××”×™×¨×”)...\")\n",
    "    for doc_tokens, fname in zip(docs, filenames):\n",
    "        vector = {}\n",
    "        \n",
    "        # --- 1. ×”××•×¤×˜×™××™×–×¦×™×” ---\n",
    "        # ×¡×•×¤×¨×™× ××ª ×”××™×œ×™× ×¨×§ ×¤×¢× ××—×ª ×¢×‘×•×¨ ×”××¡××š ×”× ×•×›×—×™\n",
    "        doc_freqs = Counter(doc_tokens)\n",
    "        \n",
    "        # --- 2. ×”××•×¤×˜×™××™×–×¦×™×” ---\n",
    "        # ×¨×¦×™× *×¨×§* ×¢×œ ×”××™×œ×™× ×”×™×™×—×•×“×™×•×ª ×‘××¡××š ×–×” (×œ× ×¢×œ ×›×œ ×”××™×œ×•×Ÿ)\n",
    "        for term in doc_freqs.keys():\n",
    "            # (××™×Ÿ ×¦×•×¨×š ×‘-if f > 0 ×›×™ ×× ×—× ×• ×™×•×“×¢×™× ×©×”××™×œ×” ×§×™×™××ª)\n",
    "            f = doc_freqs[term]\n",
    "            \n",
    "            # --- ×–×”×™×¨×•×ª: `rank_bm25` ×œ× ××—×–×™×¨ ×¦×™×•×Ÿ ×œ××™×œ×™× ×©×œ× ×‘-IDF ---\n",
    "            # ××‘×œ ×× ×—× ×• ×¡×™× × ×• ××ª `docs` ×¢× `vocab` ××– ×–×” ×××•×¨ ×œ×”×™×•×ª ×‘×¡×“×¨\n",
    "            if term not in bm25.idf:\n",
    "                continue # ××™×œ×” ×–×• ×”×™×™×ª×” × ×“×™×¨×” ××“×™ ×•×”×•×¡×¨×”\n",
    "                \n",
    "            # ××•×ª×” × ×•×¡×—×” ×‘×“×™×•×§ ××”×§×•×“ ×©×œ×š\n",
    "            score = bm25.idf[term] * ((f * (bm25.k1 + 1)) / (f + bm25.k1 * (1 - bm25.b + bm25.b * len(doc_tokens) / bm25.avgdl)))\n",
    "            vector[term] = round(score, 3)\n",
    "            \n",
    "        # ×©××™×¨×ª ×§×•×‘×¥ ×”-JSON\n",
    "        with open(os.path.join(output_dir, fname.replace(\".txt\", \".json\")), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(vector, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# === ×”×¤×¢×œ×ª ×”×ª×”×œ×™×š (×¢× ×”××•×¤×˜×™××™×–×¦×™×”) ===\n",
    "base_data = \"data\"\n",
    "base_model = \"models\"\n",
    "\n",
    "# --- ×¢×™×‘×•×“ ×’×¨×¡×ª spaCy (Word) ---\n",
    "spacy_docs_raw, spacy_names = load_texts(os.path.join(base_data, \"tokenized_text_spacy\"))\n",
    "spacy_docs_filtered, spacy_vocab = filter_rare_words(spacy_docs_raw)\n",
    "save_json_vectors_optimized(spacy_docs_filtered, spacy_names, \n",
    "                            os.path.join(base_model, \"bm25_word_json_dict\"), \n",
    "                            spacy_vocab)\n",
    "print(\"--- ×¡×™×•× ×¢×™×‘×•×“ Word ---\")\n",
    "\n",
    "\n",
    "# --- ×¢×™×‘×•×“ ×’×¨×¡×ª Lemmatized (Lemm) ---\n",
    "lemm_docs_raw, lemm_names = load_texts(os.path.join(base_data, \"lemmatized_files\"))\n",
    "lemm_docs_filtered, lemm_vocab = filter_rare_words(lemm_docs_raw)\n",
    "save_json_vectors_optimized(lemm_docs_filtered, lemm_names, \n",
    "                            os.path.join(base_model, \"bm25_lemm_json_dict\"), \n",
    "                            lemm_vocab)\n",
    "print(\"--- ×¡×™×•× ×¢×™×‘×•×“ Lemm ---\")\n",
    "\n",
    "print(\"âœ… Done! All optimized vectors saved per document.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "×˜×•×¢×Ÿ ×•×× ×§×” ×˜×§×¡×˜×™× ×: data/tokenized_text_spacy\n",
      "××¡× ×Ÿ ××™×œ×™× × ×“×™×¨×•×ª...\n",
      "×’×•×“×œ ××•×¦×¨ ×”××™×œ×™× ×”××§×•×¨×™: 62557, ××—×¨×™ ×¡×™× ×•×Ÿ: 29989\n",
      "×‘×•× ×” ××•×“×œ BM25 Okapi ×¢×‘×•×¨ models/bm25_word_json_dict...\n",
      "×™×•×¦×¨ ×•×©×•××¨ ×•×§×˜×•×¨×™× (×‘×©×™×˜×” ×”××”×™×¨×”)...\n",
      "--- ×¡×™×•× ×¢×™×‘×•×“ Word ---\n",
      "×˜×•×¢×Ÿ ×•×× ×§×” ×˜×§×¡×˜×™× ×: data/lemmatized_files\n",
      "××¡× ×Ÿ ××™×œ×™× × ×“×™×¨×•×ª...\n",
      "×’×•×“×œ ××•×¦×¨ ×”××™×œ×™× ×”××§×•×¨×™: 52714, ××—×¨×™ ×¡×™× ×•×Ÿ: 23604\n",
      "×‘×•× ×” ××•×“×œ BM25 Okapi ×¢×‘×•×¨ models/bm25_lemm_json_dict...\n",
      "×™×•×¦×¨ ×•×©×•××¨ ×•×§×˜×•×¨×™× (×‘×©×™×˜×” ×”××”×™×¨×”)...\n",
      "--- ×¡×™×•× ×¢×™×‘×•×“ Lemm ---\n",
      "âœ… Done! All optimized vectors saved per document.\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section F: Word2Vec Document Vectors with Optimizations\n",
    "In this section we will create Word2Vec document vectors with optimizations.\n",
    "The optimizations include:\n",
    "1. **Multi-core Processing**: We will utilize all available CPU cores to speed up the training of the Word2Vec model by setting the `workers` parameter.\n",
    "2. **Efficient Document Vector Calculation**: We will compute document vectors by averaging only the word vectors of words that exist in the Word2Vec vocabulary, avoiding unnecessary computations for out-of-vocabulary words."
   ],
   "id": "ed93346f79b93123"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a6e6317efe6dfc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T12:43:17.966279Z",
     "start_time": "2025-11-13T12:41:19.084357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec | ××¤×¢×™×œ ××•×¤×˜×™××™×–×¦×™×”: ××©×ª××© ×‘-8 ×œ×™×‘×•×ª CPU.\n",
      "\n",
      "××ª×—×™×œ ××•×“×œ 1/4: Word (×¢× stop-words)\n",
      "\n",
      "××ª×—×™×œ ××•×“×œ 2/4: Word (×‘×œ×™ stop-words)\n",
      "\n",
      "××ª×—×™×œ ××•×“×œ 3/4: Lemma (×¢× stop-words)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "××ª×—×™×œ ××•×“×œ 4/4: Lemma (×‘×œ×™ stop-words)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Done! 4 folders with 300-dimensional Word2Vec document vectors created.\n"
     ]
    }
   ],
   "source": [
    "import os  # ğŸš€ ×”×•×¡×¤× ×• ××ª ×–×”\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import sys\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "# --- Auto-install gensim if not found ---\n",
    "if importlib.util.find_spec(\"gensim\") is None:\n",
    "    print(\"Installing gensim ...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"gensim\"])\n",
    "\n",
    "# Now safe to import\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# --- Downloads for NLTK ---\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "# --- Utility functions ---\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Remove punctuation, digits, quotes, etc.\"\"\"\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \" \", text)      # remove numbers\n",
    "    text = text.lower().strip()\n",
    "    return text\n",
    "\n",
    "def load_docs(folder, remove_stop=False):\n",
    "    \"\"\"Load all txt files and tokenize\"\"\"\n",
    "    docs, names = [], []\n",
    "    for path in glob.glob(os.path.join(folder, \"*.txt\")):\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = clean_text(f.read())\n",
    "            tokens = [w for w in word_tokenize(text) if w.isalpha()]\n",
    "            if remove_stop:\n",
    "                tokens = [w for w in tokens if w not in STOP_WORDS]\n",
    "            docs.append(tokens)\n",
    "            names.append(os.path.basename(path))\n",
    "    return docs, names\n",
    "\n",
    "def build_doc_vectors(docs, model):\n",
    "    \"\"\"Average all word vectors in each document\"\"\"\n",
    "    vectors = []\n",
    "    for tokens in docs:\n",
    "        valid = [w for w in tokens if w in model.wv]\n",
    "        if valid:\n",
    "            vecs = np.array([model.wv[w] for w in valid])\n",
    "            vectors.append(np.mean(vecs, axis=0))\n",
    "        else:\n",
    "            vectors.append(np.zeros(model.vector_size))\n",
    "    return vectors\n",
    "\n",
    "def save_vectors(vectors, names, out_dir):\n",
    "    \"\"\"Save each document vector as its own JSON\"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for vec, name in zip(vectors, names):\n",
    "        with open(os.path.join(out_dir, name.replace(\".txt\", \".json\")), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({\"vector\": vec.tolist()}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# --- Main Configuration ---\n",
    "base_data = \"data\"\n",
    "base_model = \"models\"\n",
    "VECTOR_SIZE = 300\n",
    "\n",
    "# --- ğŸš€ ××•×¤×˜×™××™×–×¦×™×”: ×©×™××•×© ×‘×›×œ ×”×œ×™×‘×•×ª ×”×–××™× ×•×ª ---\n",
    "CPU_CORES = os.cpu_count() or 1 # ( or 1 ×œ××§×¨×” ×©×”×¤×§×•×“×” × ×›×©×œ×ª)\n",
    "print(f\"Word2Vec | ××¤×¢×™×œ ××•×¤×˜×™××™×–×¦×™×”: ××©×ª××© ×‘-{CPU_CORES} ×œ×™×‘×•×ª CPU.\")\n",
    "\n",
    "\n",
    "# --- 1ï¸âƒ£ tokenized_text_spacy - with stopwords ---\n",
    "print(\"\\n××ª×—×™×œ ××•×“×œ 1/4: Word (×¢× stop-words)\")\n",
    "docs, names = load_docs(os.path.join(base_data, \"tokenized_text_spacy\"), remove_stop=False)\n",
    "model = Word2Vec(sentences=docs, vector_size=VECTOR_SIZE, window=5, min_count=2, workers=CPU_CORES) # ğŸš€\n",
    "save_vectors(build_doc_vectors(docs, model), names, os.path.join(base_model, \"w2v_word_with_stop\"))\n",
    "\n",
    "# --- 2ï¸âƒ£ tokenized_text_spacy - no stopwords ---\n",
    "print(\"\\n××ª×—×™×œ ××•×“×œ 2/4: Word (×‘×œ×™ stop-words)\")\n",
    "docs_ns, names_ns = load_docs(os.path.join(base_data, \"tokenized_text_spacy\"), remove_stop=True)\n",
    "model_ns = Word2Vec(sentences=docs_ns, vector_size=VECTOR_SIZE, window=5, min_count=2, workers=CPU_CORES) # ğŸš€\n",
    "save_vectors(build_doc_vectors(docs_ns, model_ns), names_ns, os.path.join(base_model, \"w2v_word_no_stop\"))\n",
    "\n",
    "# --- 3ï¸âƒ£ lemmatized_text - with stopwords ---\n",
    "print(\"\\n××ª×—×™×œ ××•×“×œ 3/4: Lemma (×¢× stop-words)\")\n",
    "lemm_docs, lemm_names = load_docs(os.path.join(base_data, \"lemmatized_files\"), remove_stop=False)\n",
    "model_lemm = Word2Vec(sentences=lemm_docs, vector_size=VECTOR_SIZE, window=5, min_count=2, workers=CPU_CORES) # ğŸš€\n",
    "save_vectors(build_doc_vectors(lemm_docs, model_lemm), lemm_names, os.path.join(base_model, \"w2v_lemm_with_stop\"))\n",
    "\n",
    "# --- 4ï¸âƒ£ lemmatized_text - no stopwords ---\n",
    "print(\"\\n××ª×—×™×œ ××•×“×œ 4/4: Lemma (×‘×œ×™ stop-words)\")\n",
    "lemm_docs_ns, lemm_names_ns = load_docs(os.path.join(base_data, \"lemmatized_files\"), remove_stop=True)\n",
    "model_lemm_ns = Word2Vec(sentences=lemm_docs_ns, vector_size=VECTOR_SIZE, window=5, min_count=2, workers=CPU_CORES) # ğŸš€\n",
    "save_vectors(build_doc_vectors(lemm_docs_ns, model_lemm_ns), lemm_names_ns, os.path.join(base_model, \"w2v_lemm_no_stop\"))\n",
    "\n",
    "print(\"\\nâœ… Done! 4 folders with 300-dimensional Word2Vec document vectors created.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section G: SimCSE Document Vectors\n",
    "here we will create document vectors using the SimCSE model from the SentenceTransformers library.\n",
    "what it does is that it takes in sentences and produces high-quality sentence embeddings that capture semantic meaning.\n",
    "its different from traditional methods like TF-IDF or Word2Vec because it uses deep learning to understand context."
   ],
   "id": "4e6798a97132f7a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb814ccbfa587ccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T12:37:15.591481Z",
     "start_time": "2025-11-13T12:37:15.591414Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"--- ×”×ª×—×œ×ª ×¡×¢×™×£ ×’': SimCSE ---\")\n",
    "\n",
    "# --- 1. ×”×’×“×¨×ª × ×ª×™×‘×™× ---\n",
    "INPUT_DIR = os.path.join('data', 'combined_xml_files') \n",
    "OUTPUT_DIR = os.path.join('models', 'simcse_origen')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 2. ×‘×“×™×§×ª ×”××¦×ª GPU (MPS) ×¢×‘×•×¨ M1/M2 ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"×–×™×”×•×™ M1/M2 GPU (MPS). ××¤×¢×™×œ ×”××¦×”.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"×–×™×”×•×™ NVIDIA GPU (CUDA). ××¤×¢×™×œ ×”××¦×”.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"×œ× ×–×•×”×ª×” ×”××¦×ª GPU. ××©×ª××© ×‘-CPU.\")\n",
    "\n",
    "# --- 3. ×˜×¢×™× ×ª ××•×“×œ SimCSE ---\n",
    "model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'\n",
    "print(f\"×˜×•×¢×Ÿ ××ª ×”××•×“×œ {model_name}...\")\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "print(\"×”××•×“×œ × ×˜×¢×Ÿ.\")\n",
    "\n",
    "# --- 4. ×˜×¢×™× ×ª ×”××¡××›×™× ---\n",
    "all_texts = []\n",
    "all_names = []\n",
    "print(f\"×˜×•×¢×Ÿ ××ª ×›×œ ×§×‘×¦×™ ×”××§×•×¨ ×-{INPUT_DIR}...\")\n",
    "for file_path in glob.glob(os.path.join(INPUT_DIR, \"*.txt\")):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_texts.append(f.read())\n",
    "    all_names.append(os.path.basename(file_path))\n",
    "print(f\"× ××¦××• {len(all_texts)} ××¡××›×™×.\")\n",
    "\n",
    "# --- 5. ×™×¦×™×¨×ª ×•×§×˜×•×¨×™× (Embeddings) ---\n",
    "print(\"××ª×—×™×œ ×‘×™×¦×™×¨×ª ×•×§×˜×•×¨×™ SimCSE...\")\n",
    "vectors = model.encode(\n",
    "    all_texts, \n",
    "    show_progress_bar=True, \n",
    "    batch_size=32, # ××¤×©×¨ ×œ×”×’×“×™×œ ×× ×™×© ×œ×š ×”×¨×‘×” VRAM\n",
    "    convert_to_numpy=True \n",
    ")\n",
    "print(\"×™×¦×™×¨×ª ×”×•×§×˜×•×¨×™× ×”×•×©×œ××”.\")\n",
    "\n",
    "# --- 6. ×©××™×¨×ª ×”×•×§×˜×•×¨×™× ---\n",
    "print(f\"×©×•××¨ ××ª ×”×•×§×˜×•×¨×™× ×‘-{OUTPUT_DIR}...\")\n",
    "for vec, name in zip(vectors, all_names):\n",
    "    output_path = os.path.join(OUTPUT_DIR, name.replace(\".txt\", \".json\"))\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"vector\": vec.tolist()}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… ×¡×™×•× ×¡×¢×™×£ ×’'! ×•×§×˜×•×¨×™ SimCSE × ×©××¨×•.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section H: SBERT Document Vectors\n",
    "here we will create document vectors using the SBERT (Sentence-BERT) model from the SentenceTransformers library.\n",
    "it works similarly to SimCSE but is trained specifically for producing high-quality sentence embeddings.\n",
    "it differs from simCSE in that it uses a siamese network architecture to directly optimize for sentence similarity tasks.\n",
    "siamese networks consist of two identical subnetworks that share weights and are trained to minimize the distance between similar sentences while maximizing the distance between dissimilar ones."
   ],
   "id": "5fcb1b92a955c32c"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7be0387062493cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T12:18:35.213354Z",
     "start_time": "2025-11-13T12:17:45.135230Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ×”×ª×—×œ×ª ×¡×¢×™×£ ×“': SBERT ---\n",
      "×–×™×”×•×™ M1/M2 GPU (MPS). ××¤×¢×™×œ ×”××¦×”.\n",
      "×˜×•×¢×Ÿ ××ª ×”××•×“×œ all-mpnet-base-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b18419422c404593c548cf6afa965a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4884f88d784d2da6e7be3083444cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256e2f3bbb7c46bf8fef11fb0d7fe316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d7bd4205f9471982a81b27764f2539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22887d19faf0496b9eb4b9dd09c5d74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d7adb810a04e7690fe77c2f40586b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de7268280c04719a2c5f81656755b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624470a5760946fda5d29e6f8415dc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eecbafbbb354098a0b68f460d73c79b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06123c8130344ecace8b35750d6ba02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa7503e820046429215ebb393816305",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "×”××•×“×œ × ×˜×¢×Ÿ.\n",
      "×˜×•×¢×Ÿ ××ª ×›×œ ×§×‘×¦×™ ×”××§×•×¨ ×-data/combined_xml_files...\n",
      "× ××¦××• 336 ××¡××›×™×.\n",
      "××ª×—×™×œ ×‘×™×¦×™×¨×ª ×•×§×˜×•×¨×™ SBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c78316631c6427ea9f3c6ec965136c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "×™×¦×™×¨×ª ×”×•×§×˜×•×¨×™× ×”×•×©×œ××”.\n",
      "×©×•××¨ ××ª ×”×•×§×˜×•×¨×™× ×‘-models/sbert_origen...\n",
      "âœ… ×¡×™×•× ×¡×¢×™×£ ×“'! ×•×§×˜×•×¨×™ SBERT × ×©××¨×•.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"--- ×”×ª×—×œ×ª ×¡×¢×™×£ ×“': SBERT ---\")\n",
    "\n",
    "# --- 1. ×”×’×“×¨×ª × ×ª×™×‘×™× ---\n",
    "# ×©×™××•×© ×‘××•×ª× ×§×‘×¦×™ ××§×•×¨ ×›××• SimCSE\n",
    "INPUT_DIR = os.path.join('data', 'combined_xml_files') \n",
    "# ×ª×™×§×™×™×ª ×¤×œ×˜ ×œ×¤×™ ×©× ×”×§×‘×•×¦×” ×‘××˜×œ×”\n",
    "OUTPUT_DIR = os.path.join('models', 'sbert_origen')\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- 2. ×‘×“×™×§×ª ×”××¦×ª GPU (MPS) ---\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"×–×™×”×•×™ M1/M2 GPU (MPS). ××¤×¢×™×œ ×”××¦×”.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"×–×™×”×•×™ NVIDIA GPU (CUDA). ××¤×¢×™×œ ×”××¦×”.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"×œ× ×–×•×”×ª×” ×”××¦×ª GPU. ××©×ª××© ×‘-CPU.\")\n",
    "\n",
    "# --- 3. ×˜×¢×™× ×ª ××•×“×œ SBERT ---\n",
    "# ×–×”×• ××•×“×œ SBERT ×¤×•×¤×•×œ×¨×™ ×•××•××œ×¥ ×œ×©×™××•×© ×›×œ×œ×™\n",
    "model_name = 'all-mpnet-base-v2'\n",
    "print(f\"×˜×•×¢×Ÿ ××ª ×”××•×“×œ {model_name}...\")\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "print(\"×”××•×“×œ × ×˜×¢×Ÿ.\")\n",
    "\n",
    "# --- 4. ×˜×¢×™× ×ª ×”××¡××›×™× ---\n",
    "# ×”×§×•×“ ×–×”×” ×œ×¡×¢×™×£ ×”×§×•×“×\n",
    "all_texts = []\n",
    "all_names = []\n",
    "print(f\"×˜×•×¢×Ÿ ××ª ×›×œ ×§×‘×¦×™ ×”××§×•×¨ ×-{INPUT_DIR}...\")\n",
    "for file_path in glob.glob(os.path.join(INPUT_DIR, \"*.txt\")):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        all_texts.append(f.read())\n",
    "    all_names.append(os.path.basename(file_path))\n",
    "print(f\"× ××¦××• {len(all_texts)} ××¡××›×™×.\")\n",
    "\n",
    "# --- 5. ×™×¦×™×¨×ª ×•×§×˜×•×¨×™× (Embeddings) ---\n",
    "print(\"××ª×—×™×œ ×‘×™×¦×™×¨×ª ×•×§×˜×•×¨×™ SBERT...\")\n",
    "vectors = model.encode(\n",
    "    all_texts, \n",
    "    show_progress_bar=True, \n",
    "    batch_size=32,\n",
    "    convert_to_numpy=True \n",
    ")\n",
    "print(\"×™×¦×™×¨×ª ×”×•×§×˜×•×¨×™× ×”×•×©×œ××”.\")\n",
    "\n",
    "# --- 6. ×©××™×¨×ª ×”×•×§×˜×•×¨×™× ---\n",
    "print(f\"×©×•××¨ ××ª ×”×•×§×˜×•×¨×™× ×‘-{OUTPUT_DIR}...\")\n",
    "for vec, name in zip(vectors, all_names):\n",
    "    output_path = os.path.join(OUTPUT_DIR, name.replace(\".txt\", \".json\"))\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"vector\": vec.tolist()}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… ×¡×™×•× ×¡×¢×™×£ ×“'! ×•×§×˜×•×¨×™ SBERT × ×©××¨×•.\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7ba44c297335bf52"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section I1: we used kmeans with the best k  + BM25 Features for Advanced Analysis with mutual_info_classif and Chi2 and gini to find important features\n",
    "\n"
   ],
   "id": "32585ad42a4f4a64"
  },
  {
   "cell_type": "code",
   "id": "8e3e1933fb5da882",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:26:54.498467Z",
     "start_time": "2025-11-19T10:26:15.457886Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif, chi2\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. ×ª×¦×•×¨×” ---\n",
    "print(\"--- ××ª×—×™×œ ×¤×ª×¨×•×Ÿ 1: SVD + K-Means ---\")\n",
    "\n",
    "# !!! ×©× ×” ××ª ×”× ×ª×™×‘ ×‘×™×Ÿ ×¨×™×¦×•×ª\n",
    "TFIDF_DIR = os.path.join('models', 'bm25_word_json_dict')\n",
    "#TFIDF_DIR = os.path.join('models', 'bm25_lemm_json_dict')\n",
    "\n",
    "# × ×¦××¦× ××ª 19,000+ ×”××™××“×™× ×œ-50 \"××•×©×’×™×\"\n",
    "N_SVD_COMPONENTS = 50\n",
    "\n",
    "# ×‘××§×•× Silhouette × ×‘×—×¨ K ×™×“× ×™. ×ª×©× ×” ×œ×¤×™ ××” ×©××¢× ×™×™×Ÿ ××•×ª×š.\n",
    "BEST_K = 5\n",
    "\n",
    "print(f\"××¢×‘×“ ××˜×¨×™×¦×”: {TFIDF_DIR}\")\n",
    "print(f\"×™×¦××¦× ××™××“×™× ×œ- {N_SVD_COMPONENTS} ×¨×›×™×‘×™×.\")\n",
    "print(f\"××©×ª××© ×‘-K ×§×‘×•×¢: {BEST_K}\")\n",
    "\n",
    "# --- 2. ×˜×¢×™× ×ª × ×ª×•× ×™× ---\n",
    "doc_vectors = []\n",
    "filenames = []\n",
    "print(\"×˜×•×¢×Ÿ ×§×‘×¦×™ JSON...\")\n",
    "for file_path in glob.glob(os.path.join(TFIDF_DIR, \"*.json\")):\n",
    "    fname = os.path.basename(file_path)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        doc_vectors.append(json.load(f))\n",
    "    filenames.append(fname)\n",
    "print(f\"× ××¦××• {len(doc_vectors)} ××¡××›×™×.\")\n",
    "\n",
    "# --- 3. ×‘× ×™×™×ª ××˜×¨×™×¦×” ---\n",
    "print(\"×××™×¨ ×œ-feature matrix (X_sparse)...\")\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_sparse = vectorizer.fit_transform(doc_vectors)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"××˜×¨×™×¦×” ×“×œ×™×œ×” × ×•×¦×¨×” ×‘×’×•×“×œ: {X_sparse.shape}\")\n",
    "\n",
    "# --- 4. ×¦××¦×•× ××™××“×™× ×¢× SVD ---\n",
    "print(f\"××‘×¦×¢ Truncated SVD ×œ×¦××¦×•× ××™××“×™× ×œ- {N_SVD_COMPONENTS} ×¨×›×™×‘×™×...\")\n",
    "svd = TruncatedSVD(n_components=N_SVD_COMPONENTS, random_state=42)\n",
    "X_reduced = svd.fit_transform(X_sparse)\n",
    "print(f\"××˜×¨×™×¦×” ××¦×•××¦××ª × ×•×¦×¨×” ×‘×’×•×“×œ: {X_reduced.shape}\")\n",
    "\n",
    "# --- 5. ×”×¨×¦×” ×©×œ K-Means ×¢× K ×§×‘×•×¢ ---\n",
    "print(f\"\\n--- 5. ×”×¨×¦×” ×©×œ K-Means ×¢× K={BEST_K} ---\")\n",
    "kmeans = KMeans(n_clusters=BEST_K, random_state=42, n_init=10)\n",
    "y = kmeans.fit_predict(X_reduced)\n",
    "cluster_counts = np.bincount(y)\n",
    "print(f\"×”×ª×¤×œ×’×•×ª ×¡×•×¤×™×ª ×‘- {BEST_K} ×§×‘×•×¦×•×ª:\")\n",
    "print(cluster_counts)\n",
    "\n",
    "# --- 6. × ×™×ª×•×— ×—×©×™×‘×•×ª ×¤×™×¦'×¨×™× (××•×œ X_sparse ×”××§×•×¨×™) ---\n",
    "print(f\"\\n--- 6. ×—×™×©×•×‘ ×—×©×™×‘×•×ª ×¤×™×¦'×¨×™× (××•×œ X_sparse ×”××§×•×¨×™) ---\")\n",
    "\n",
    "print(f\"[K={BEST_K}] ××—×©×‘ Information Gain...\")\n",
    "ig_results = pd.DataFrame()\n",
    "try:\n",
    "    print(\"  (×××™×¨ ×œ×¦×¤×•×£...)\")\n",
    "    X_dense = X_sparse.toarray()\n",
    "    ig_scores = mutual_info_classif(X_dense, y, discrete_features=False)\n",
    "    ig_results = pd.DataFrame({'feature': feature_names, 'info_gain': ig_scores})\n",
    "    ig_results = ig_results.sort_values(by='info_gain', ascending=False)\n",
    "    print(\"  ×—×™×©×•×‘ Info Gain ×”×¦×œ×™×—.\")\n",
    "except MemoryError:\n",
    "    print(\"  !!! ×©×’×™××ª ×–×™×›×¨×•×Ÿ. ××“×œ×’ ×¢×œ Info Gain.\")\n",
    "except Exception as e:\n",
    "    print(f\"  !!! ×©×’×™××” ×‘-Info Gain: {e}. ××“×œ×’.\")\n",
    "\n",
    "print(f\"[K={BEST_K}] ××—×©×‘ Chi-squared...\")\n",
    "chi2_scores, p_values = chi2(X_sparse, y)\n",
    "chi2_results = pd.DataFrame({'feature': feature_names,\n",
    "                             'chi2_score': chi2_scores,\n",
    "                             'p_value': p_values})\n",
    "chi2_results = chi2_results.sort_values(by='chi2_score', ascending=False)\n",
    "print(\"  ×—×™×©×•×‘ Chi-squared ×”×¦×œ×™×—.\")\n",
    "\n",
    "# --- 7. ×©××™×¨×ª ×”×ª×•×¦××•×ª ×œ××§×¡×œ ---\n",
    "output_filename = f\"first_method_word_{os.path.basename(TFIDF_DIR)}.xlsx\"\n",
    "print(f\"\\n--- 7. ×©×•××¨ ×”×›×œ ×œ×§×•×‘×¥ ××§×¡×œ ××—×“ ---\")\n",
    "print(f\"×§×•×‘×¥: {output_filename}\")\n",
    "\n",
    "with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "    # ×’×™×œ×™×•× ×•×ª ××œ××™×\n",
    "    if not ig_results.empty:\n",
    "        ig_results.to_excel(writer, sheet_name='Information Gain (Full)', index=False)\n",
    "    chi2_results.to_excel(writer, sheet_name='Chi-squared (Full)', index=False)\n",
    "\n",
    "    # ×˜×•×¤ 20\n",
    "    if not ig_results.empty:\n",
    "        ig_results.head(20).to_excel(writer, sheet_name='Top 20 Info Gain', index=False)\n",
    "    chi2_results.head(20).to_excel(writer, sheet_name='Top 20 Chi-squared', index=False)\n",
    "\n",
    "print(\"\\n\\nâœ…âœ…âœ… ×¡×™×•× ×¤×ª×¨×•×Ÿ 1 (×œ×œ× Silhouette, ×œ×œ× ×’×¨×¤×™×, ×œ×œ× Gini)! âœ…âœ…âœ…\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ××ª×—×™×œ ×¤×ª×¨×•×Ÿ 1: SVD + K-Means ---\n",
      "××¢×‘×“ ××˜×¨×™×¦×”: models/bm25_word_json_dict\n",
      "×™×¦××¦× ××™××“×™× ×œ- 50 ×¨×›×™×‘×™×.\n",
      "××©×ª××© ×‘-K ×§×‘×•×¢: 5\n",
      "×˜×•×¢×Ÿ ×§×‘×¦×™ JSON...\n",
      "× ××¦××• 336 ××¡××›×™×.\n",
      "×××™×¨ ×œ-feature matrix (X_sparse)...\n",
      "××˜×¨×™×¦×” ×“×œ×™×œ×” × ×•×¦×¨×” ×‘×’×•×“×œ: (336, 29989)\n",
      "××‘×¦×¢ Truncated SVD ×œ×¦××¦×•× ××™××“×™× ×œ- 50 ×¨×›×™×‘×™×...\n",
      "××˜×¨×™×¦×” ××¦×•××¦××ª × ×•×¦×¨×” ×‘×’×•×“×œ: (336, 50)\n",
      "\n",
      "--- 5. ×”×¨×¦×” ×©×œ K-Means ×¢× K=5 ---\n",
      "×”×ª×¤×œ×’×•×ª ×¡×•×¤×™×ª ×‘- 5 ×§×‘×•×¦×•×ª:\n",
      "[ 47  11  52 225   1]\n",
      "\n",
      "--- 6. ×—×™×©×•×‘ ×—×©×™×‘×•×ª ×¤×™×¦'×¨×™× (××•×œ X_sparse ×”××§×•×¨×™) ---\n",
      "[K=5] ××—×©×‘ Information Gain...\n",
      "  (×××™×¨ ×œ×¦×¤×•×£...)\n",
      "  ×—×™×©×•×‘ Info Gain ×”×¦×œ×™×—.\n",
      "[K=5] ××—×©×‘ Chi-squared...\n",
      "  ×—×™×©×•×‘ Chi-squared ×”×¦×œ×™×—.\n",
      "\n",
      "--- 7. ×©×•××¨ ×”×›×œ ×œ×§×•×‘×¥ ××§×¡×œ ××—×“ ---\n",
      "×§×•×‘×¥: first_method_word_bm25_word_json_dict.xlsx\n",
      "\n",
      "\n",
      "âœ…âœ…âœ… ×¡×™×•× ×¤×ª×¨×•×Ÿ 1 (×œ×œ× Silhouette, ×œ×œ× ×’×¨×¤×™×, ×œ×œ× Gini)! âœ…âœ…âœ…\n"
     ]
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Section I2: NMF Topic Modeling with BM25 Features",
   "id": "3175ee16016390d7"
  },
  {
   "cell_type": "code",
   "id": "fed2b19fadca96f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:28:21.057467Z",
     "start_time": "2025-11-19T10:28:17.440919Z"
    }
   },
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "print(\"--- Starting NMF Topic Modeling (simplified) ---\")\n",
    "\n",
    "# !!! UPDATE THIS PATH to point to your JSON folder\n",
    "TFIDF_DIR = os.path.join('models', 'bm25_lemm_json_dict')\n",
    "\n",
    "# Parameters\n",
    "BEST_K = 5         # ××¡×¤×¨ ×”× ×•×©××™× (topics) â€“ ×ª×§×‘×¢ ×™×“× ×™×ª\n",
    "N_TOP_WORDS = 20     # ××¡×¤×¨ ×”××™×œ×™× ×œ×›×œ topic\n",
    "\n",
    "print(f\"Target Directory: {TFIDF_DIR}\")\n",
    "print(f\"Using fixed K (Topics): {BEST_K}\")\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "doc_vectors = []\n",
    "filenames = []\n",
    "\n",
    "print(\"Loading JSON files...\")\n",
    "json_files = glob.glob(os.path.join(TFIDF_DIR, \"*.json\"))\n",
    "\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"No JSON files found in {TFIDF_DIR}\")\n",
    "\n",
    "for file_path in json_files:\n",
    "    fname = os.path.basename(file_path)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        doc_vectors.append(data)\n",
    "    filenames.append(fname)\n",
    "\n",
    "print(f\"Successfully loaded {len(doc_vectors)} documents.\")\n",
    "\n",
    "# --- 3. Build Feature Matrix ---\n",
    "print(\"Vectorizing data...\")\n",
    "vectorizer = DictVectorizer(sparse=True)\n",
    "X_sparse = vectorizer.fit_transform(doc_vectors)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(f\"Matrix Shape: {X_sparse.shape} (Docs x Words)\")\n",
    "\n",
    "# --- 4. Fix for NMF: Handle Negative Values ---\n",
    "min_val = X_sparse.data.min() if X_sparse.nnz > 0 else 0\n",
    "if min_val < 0:\n",
    "    print(f\"!!! Warning: Negative values detected (Min: {min_val:.4f}). Shifting data...\")\n",
    "    X_sparse.data -= min_val\n",
    "\n",
    "# --- 5. Train Final NMF Model with Fixed K ---\n",
    "print(f\"\\n--- Training Final NMF Model with K={BEST_K} ---\")\n",
    "nmf_model = NMF(\n",
    "    n_components=BEST_K,\n",
    "    random_state=42,\n",
    "    init='nndsvda',\n",
    "    max_iter=500\n",
    ")\n",
    "\n",
    "# W: Document-Topic Matrix (Docs x Topics)\n",
    "W = nmf_model.fit_transform(X_sparse)\n",
    "\n",
    "# H: Topic-Word Matrix (Topics x Words)\n",
    "H = nmf_model.components_\n",
    "\n",
    "# Assign each document to its dominant topic\n",
    "doc_topic_labels = np.argmax(W, axis=1)\n",
    "topic_counts = np.bincount(doc_topic_labels)\n",
    "\n",
    "# --- 6. Extract Topics & Keywords ---\n",
    "print(\"Extracting keywords per topic...\")\n",
    "topic_dict = {}\n",
    "\n",
    "for topic_idx, topic_weights in enumerate(H):\n",
    "    # indices of top N words\n",
    "    top_indices = topic_weights.argsort()[:-N_TOP_WORDS - 1:-1]\n",
    "\n",
    "    top_words = [feature_names[i] for i in top_indices]\n",
    "    top_scores = [topic_weights[i] for i in top_indices]\n",
    "\n",
    "    # quick console summary\n",
    "    print(f\"Topic {topic_idx} ({topic_counts[topic_idx]} docs): {', '.join(top_words[:5])}...\")\n",
    "\n",
    "    # store for Excel\n",
    "    topic_dict[f'Topic_{topic_idx}_Words'] = top_words\n",
    "    topic_dict[f'Topic_{topic_idx}_Weights'] = top_scores\n",
    "\n",
    "topics_df = pd.DataFrame(topic_dict)\n",
    "\n",
    "# --- 7. Build Doc Mapping DataFrame ---\n",
    "mapping_df = pd.DataFrame({\n",
    "    'Filename': filenames,\n",
    "    'Predicted_Topic': doc_topic_labels,\n",
    "    'Topic_Strength': np.max(W, axis=1)  # how strongly the doc fits its main topic\n",
    "})\n",
    "\n",
    "# --- 8. Save to Excel (no plots) ---\n",
    "output_filename = f\"Second_method_lemmas_NMF_Results_.xlsx\"\n",
    "print(f\"\\n--- Saving results to: {output_filename} ---\")\n",
    "\n",
    "with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "    # Sheet 1: Topic Keywords\n",
    "    topics_df.to_excel(writer, sheet_name='Topic Keywords', index=False)\n",
    "\n",
    "    # Sheet 2: Doc Mapping\n",
    "    mapping_df.to_excel(writer, sheet_name='Doc Mapping', index=False)\n",
    "\n",
    "print(\"âœ… Processing Complete (no Silhouette search, no plots)!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting NMF Topic Modeling (simplified) ---\n",
      "Target Directory: models/bm25_lemm_json_dict\n",
      "Using fixed K (Topics): 5\n",
      "Loading JSON files...\n",
      "Successfully loaded 336 documents.\n",
      "Vectorizing data...\n",
      "Matrix Shape: (336, 23604) (Docs x Words)\n",
      "\n",
      "--- Training Final NMF Model with K=5 ---\n",
      "Extracting keywords per topic...\n",
      "Topic 0 (190 docs): tax, economy, government, budget, business...\n",
      "Topic 1 (8 docs): birthplace, championship, fc, cricket, cathedral...\n",
      "Topic 2 (43 docs): ally, military, civilian, humanitarian, unrwa...\n",
      "Topic 3 (89 docs): amendment, clause, legislation, hon, bill...\n",
      "Topic 4 (6 docs): tapp, antonia, goldsborough, ayoub, sackman...\n",
      "\n",
      "--- Saving results to: Second_method_lemmas_NMF_Results_.xlsx ---\n",
      "âœ… Processing Complete (no Silhouette search, no plots)!\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-19T10:36:00.958241Z",
     "start_time": "2025-11-19T10:35:55.020179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd  # ×œ×©××™×¨×” ×œ××§×¡×œ\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ===== ×”×’×“×¨×•×ª ×©×ª×ª××™× ×œ×¡×‘×™×‘×” ×©×œ×š =====\n",
    "\n",
    "BM25_DIR = \"models/bm25_word_json_dict\"  # ××• bm25_word_json_dict\n",
    "N_CLUSTERS = 10                          # ××¡×¤×¨ ×”×§×œ××¡×˜×¨×™× ×œ-KMeans\n",
    "MIN_DOCS_PER_TERM = 5                    # ××™× ×™××•× ××¡××›×™× ×œ××™×œ×” ×›×“×™ ×œ×—×©×‘ ×œ×” IG\n",
    "TOP_K = 100                              # ×›××” ××™×œ×™× ×¢× IG ×”×›×™ ×’×‘×•×” ×œ×”×¦×™×’\n",
    "OUT_IG_XLSX = \"models/third_method_words_term_information_gain_clusters.xlsx\"  # ×¨×§ ××§×¡×œ\n",
    "\n",
    "\n",
    "def load_bm25_vectors(bm25_dir):\n",
    "    \"\"\"\n",
    "    ×˜×•×¢×Ÿ ××ª ×›×œ ×§×‘×¦×™ ×”-BM25 JSON ××”×ª×™×§×™×™×”.\n",
    "    ××—×–×™×¨:\n",
    "        doc_vectors: dict {doc_id -> {term: score}}\n",
    "    \"\"\"\n",
    "    doc_vectors = {}\n",
    "    file_count = 0\n",
    "\n",
    "    for fname in os.listdir(bm25_dir):\n",
    "        if not fname.endswith(\".json\"):\n",
    "            continue\n",
    "        doc_id = os.path.splitext(fname)[0]\n",
    "        full_path = os.path.join(bm25_dir, fname)\n",
    "        with open(full_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            vec = json.load(f)\n",
    "        doc_vectors[doc_id] = {t: float(s) for t, s in vec.items()}\n",
    "        file_count += 1\n",
    "\n",
    "    print(f\"×˜×¢× ×• {file_count} ××¡××›×™× ×¢× BM25 ×: {bm25_dir}\")\n",
    "    return doc_vectors\n",
    "\n",
    "\n",
    "def cluster_documents(doc_vectors, n_clusters=10):\n",
    "    \"\"\"\n",
    "    ×¢×•×©×” ×§×œ××¡×˜×¨×™× ×’ ×œ××¡××›×™× ×‘×××¦×¢×•×ª KMeans ×¢×œ ×•×§×˜×•×¨×™ BM25.\n",
    "    ××—×–×™×¨:\n",
    "        doc_labels: dict {doc_id -> 'cluster_X'}\n",
    "    \"\"\"\n",
    "    doc_ids = list(doc_vectors.keys())\n",
    "    vecs = list(doc_vectors.values())\n",
    "\n",
    "    # DictVectorizer ×”×•×¤×š ×¨×©×™××ª dict-×™× ×œ××˜×¨×™×¦×” sparse\n",
    "    dv = DictVectorizer(sparse=True)\n",
    "    X = dv.fit_transform(vecs)\n",
    "\n",
    "    print(f\"××˜×¨×™×¦×ª ××¡××›×™×: {X.shape[0]} ××¡××›×™×, {X.shape[1]} ×¤×™×¦'×¨×™× (××™×œ×™×).\")\n",
    "    print(f\"××¨×™×¥ KMeans ×¢× {n_clusters} ×§×œ××¡×˜×¨×™×...\")\n",
    "\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=42,\n",
    "        n_init=10\n",
    "    )\n",
    "    cluster_ids = kmeans.fit_predict(X)\n",
    "\n",
    "    doc_labels = {}\n",
    "    for doc_id, cid in zip(doc_ids, cluster_ids):\n",
    "        doc_labels[doc_id] = f\"cluster_{cid}\"\n",
    "\n",
    "    # ×§×¦×ª ×¡×˜×˜×™×¡×˜×™×§×”\n",
    "    label_counts = Counter(doc_labels.values())\n",
    "    print(\"×”×ª×¤×œ×’×•×ª ×§×œ××¡×˜×¨×™×:\")\n",
    "    for lbl, cnt in label_counts.items():\n",
    "        print(f\"  {lbl}: {cnt} ××¡××›×™×\")\n",
    "\n",
    "    return doc_labels\n",
    "\n",
    "\n",
    "def compute_entropy(label_counts, total_docs):\n",
    "    \"\"\"\n",
    "    H(Y) = - Î£ p(y) log2 p(y)\n",
    "    \"\"\"\n",
    "    entropy = 0.0\n",
    "    for count in label_counts.values():\n",
    "        if count == 0:\n",
    "            continue\n",
    "        p = count / total_docs\n",
    "        entropy -= p * math.log2(p)\n",
    "    return entropy\n",
    "\n",
    "\n",
    "def compute_information_gain_from_clusters(doc_vectors, doc_labels,\n",
    "                                           min_docs_per_term=5):\n",
    "    \"\"\"\n",
    "    ××—×©×‘ Information Gain ×œ×›×œ ××•× ×— ×‘×™×—×¡ ×œ×§×œ××¡×˜×¨×™× (labels).\n",
    "    doc_vectors: dict {doc_id -> {term: score}}\n",
    "    doc_labels: dict {doc_id -> label}  (×œ×™×™×‘×œ = cluster_X)\n",
    "    ××—×–×™×¨:\n",
    "        term_ig: dict {term -> IG}\n",
    "    \"\"\"\n",
    "\n",
    "    # ××›×™× ×™× ×¨×©×™××” ×©×œ (doc_id, terms_set, label)\n",
    "    docs = []\n",
    "    for doc_id, vec in doc_vectors.items():\n",
    "        if doc_id not in doc_labels:\n",
    "            continue\n",
    "        terms = set(vec.keys())\n",
    "        label = doc_labels[doc_id]\n",
    "        docs.append((doc_id, terms, label))\n",
    "\n",
    "    total_docs = len(docs)\n",
    "    print(f\"×¡×š ×”×›×œ ××¡××›×™× ×¢× BM25+×§×œ××¡×˜×¨: {total_docs}\")\n",
    "\n",
    "    # ×”×ª×¤×œ×’×•×ª ×›×œ×œ×™×ª ×©×œ ×”×œ×™×™×‘×œ×™× (×”×§×œ××¡×˜×¨×™×)\n",
    "    label_counts = Counter(label for _, _, label in docs)\n",
    "\n",
    "    base_entropy = compute_entropy(label_counts, total_docs)\n",
    "    print(f\"H(Y) (×× ×˜×¨×•×¤×™×™×ª ×§×œ××¡×˜×¨×™×): {base_entropy:.4f}\")\n",
    "\n",
    "    # term_label_presence[term][label] = ×›××” ××¡××›×™× ×¢× ×”××•× ×— ×•×¢× ×”-label\n",
    "    term_label_presence = defaultdict(Counter)\n",
    "    # term_doc_count[term] = ×›××” ××¡××›×™× ×©×‘×”× ×”××•× ×— ××•×¤×™×¢\n",
    "    term_doc_count = Counter()\n",
    "\n",
    "    for _, terms, label in docs:\n",
    "        for term in terms:\n",
    "            term_label_presence[term][label] += 1\n",
    "            term_doc_count[term] += 1\n",
    "\n",
    "    term_ig = {}\n",
    "\n",
    "    for term, present_count in term_doc_count.items():\n",
    "        if present_count < min_docs_per_term:\n",
    "            continue\n",
    "\n",
    "        N1 = present_count        # ××¡×¤×¨ ××¡××›×™× ×¢× ×”××•× ×— (X=1)\n",
    "        N0 = total_docs - N1      # ××¡×¤×¨ ××¡××›×™× ×‘×œ×™ ×”××•× ×— (X=0)\n",
    "\n",
    "        if N0 == 0 or N1 == 0:\n",
    "            continue\n",
    "\n",
    "        # H(Y | X=1)\n",
    "        entropy_y_given_x1 = 0.0\n",
    "        for label, label_total_count in label_counts.items():\n",
    "            N11 = term_label_presence[term][label]\n",
    "            if N11 == 0:\n",
    "                continue\n",
    "            p_y_given_x1 = N11 / N1\n",
    "            entropy_y_given_x1 -= p_y_given_x1 * math.log2(p_y_given_x1)\n",
    "\n",
    "        # H(Y | X=0)\n",
    "        entropy_y_given_x0 = 0.0\n",
    "        for label, label_total_count in label_counts.items():\n",
    "            N11 = term_label_presence[term][label]\n",
    "            N01 = label_total_count - N11  # ××¡××›×™× ×¢× label ××‘×œ ×‘×œ×™ term\n",
    "            if N01 == 0:\n",
    "                continue\n",
    "            p_y_given_x0 = N01 / N0\n",
    "            entropy_y_given_x0 -= p_y_given_x0 * math.log2(p_y_given_x0)\n",
    "\n",
    "        p_x1 = N1 / total_docs\n",
    "        p_x0 = N0 / total_docs\n",
    "\n",
    "        cond_entropy = p_x1 * entropy_y_given_x1 + p_x0 * entropy_y_given_x0\n",
    "        ig = base_entropy - cond_entropy\n",
    "        term_ig[term] = ig\n",
    "\n",
    "    return term_ig\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1. ×˜×•×¢× ×™× BM25\n",
    "    doc_vectors = load_bm25_vectors(BM25_DIR)\n",
    "\n",
    "    # 2. ××¨×™×¦×™× ×§×œ××¡×˜×¨×™× ×’ ×•××¤×™×§×™× pseudo-labels\n",
    "    doc_labels = cluster_documents(doc_vectors, n_clusters=N_CLUSTERS)\n",
    "\n",
    "    # 3. ××—×©×‘×™× Information Gain ×œ×›×œ ××•× ×— ×‘×™×—×¡ ×œ×§×œ××¡×˜×¨×™×\n",
    "    term_ig = compute_information_gain_from_clusters(\n",
    "        doc_vectors,\n",
    "        doc_labels,\n",
    "        min_docs_per_term=MIN_DOCS_PER_TERM\n",
    "    )\n",
    "    print(f\"× ××¦××• {len(term_ig)} ××™×œ×™× ×¢× IG ××—×¨×™ ×¡×™× ×•×Ÿ.\")\n",
    "\n",
    "    # 4. ××™×•×Ÿ ×•×”×¦×’×ª TOP K\n",
    "    sorted_terms = sorted(term_ig.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    print(f\"\\nTop {TOP_K} ××™×œ×™× ×œ×¤×™ Information Gain (×§×œ××¡×˜×¨×™× ×›-labels):\\n\")\n",
    "    for term, ig in sorted_terms[:TOP_K]:\n",
    "        print(f\"{term:25s}  IG = {ig:.5f}\")\n",
    "\n",
    "    # 5. ×©××™×¨×” ×¨×§ ×œ-Excel\n",
    "    os.makedirs(os.path.dirname(OUT_IG_XLSX), exist_ok=True)\n",
    "    df = pd.DataFrame(sorted_terms, columns=[\"term\", \"information_gain\"])\n",
    "    df.to_excel(OUT_IG_XLSX, index=False)\n",
    "    print(f\"\\nğŸ“Š ×©××¨×ª×™ ××ª ×ª×•×¦××•×ª ×”-IG ×œ××§×¡×œ ×‘: {OUT_IG_XLSX}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "4185bb4cb29f9bac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "×˜×¢× ×• 336 ××¡××›×™× ×¢× BM25 ×: models/bm25_word_json_dict\n",
      "××˜×¨×™×¦×ª ××¡××›×™×: 336 ××¡××›×™×, 29989 ×¤×™×¦'×¨×™× (××™×œ×™×).\n",
      "××¨×™×¥ KMeans ×¢× 10 ×§×œ××¡×˜×¨×™×...\n",
      "×”×ª×¤×œ×’×•×ª ×§×œ××¡×˜×¨×™×:\n",
      "  cluster_0: 150 ××¡××›×™×\n",
      "  cluster_3: 172 ××¡××›×™×\n",
      "  cluster_4: 1 ××¡××›×™×\n",
      "  cluster_2: 7 ××¡××›×™×\n",
      "  cluster_8: 1 ××¡××›×™×\n",
      "  cluster_1: 1 ××¡××›×™×\n",
      "  cluster_6: 1 ××¡××›×™×\n",
      "  cluster_9: 1 ××¡××›×™×\n",
      "  cluster_5: 1 ××¡××›×™×\n",
      "  cluster_7: 1 ××¡××›×™×\n",
      "×¡×š ×”×›×œ ××¡××›×™× ×¢× BM25+×§×œ××¡×˜×¨: 336\n",
      "H(Y) (×× ×˜×¨×•×¤×™×™×ª ×§×œ××¡×˜×¨×™×): 1.3051\n",
      "× ××¦××• 26734 ××™×œ×™× ×¢× IG ××—×¨×™ ×¡×™× ×•×Ÿ.\n",
      "\n",
      "Top 100 ××™×œ×™× ×œ×¤×™ Information Gain (×§×œ××¡×˜×¨×™× ×›-labels):\n",
      "\n",
      "intent                     IG = 0.10532\n",
      "nationals                  IG = 0.10335\n",
      "damaging                   IG = 0.09814\n",
      "wage                       IG = 0.09279\n",
      "consumers                  IG = 0.09157\n",
      "deter                      IG = 0.09108\n",
      "buy                        IG = 0.09102\n",
      "inflation                  IG = 0.09093\n",
      "drafted                    IG = 0.08850\n",
      "budget                     IG = 0.08668\n",
      "roof                       IG = 0.08633\n",
      "diplomatic                 IG = 0.08498\n",
      "exchequer                  IG = 0.08477\n",
      "coal                       IG = 0.08447\n",
      "farms                      IG = 0.08416\n",
      "cheese                     IG = 0.08395\n",
      "hit                        IG = 0.08333\n",
      "detention                  IG = 0.08306\n",
      "rises                      IG = 0.08306\n",
      "exercising                 IG = 0.08289\n",
      "doorkeeper                 IG = 0.08244\n",
      "specify                    IG = 0.08177\n",
      "taxation                   IG = 0.08100\n",
      "fondly                     IG = 0.08078\n",
      "hitting                    IG = 0.08010\n",
      "terrorists                 IG = 0.07996\n",
      "compelling                 IG = 0.07983\n",
      "dental                     IG = 0.07955\n",
      "allies                     IG = 0.07922\n",
      "connectivity               IG = 0.07908\n",
      "iran                       IG = 0.07876\n",
      "guilty                     IG = 0.07862\n",
      "dentistry                  IG = 0.07861\n",
      "grid                       IG = 0.07830\n",
      "prices                     IG = 0.07800\n",
      "revolution                 IG = 0.07765\n",
      "lancashire                 IG = 0.07743\n",
      "shops                      IG = 0.07734\n",
      "buses                      IG = 0.07721\n",
      "lifeblood                  IG = 0.07699\n",
      "terrorist                  IG = 0.07691\n",
      "taxed                      IG = 0.07687\n",
      "clauses                    IG = 0.07664\n",
      "sewage                     IG = 0.07660\n",
      "renewable                  IG = 0.07613\n",
      "base                       IG = 0.07589\n",
      "economics                  IG = 0.07578\n",
      "nationality                IG = 0.07577\n",
      "forecasts                  IG = 0.07560\n",
      "affirmative                IG = 0.07554\n",
      "tourism                    IG = 0.07522\n",
      "wages                      IG = 0.07512\n",
      "satisfied                  IG = 0.07498\n",
      "turbines                   IG = 0.07492\n",
      "escalate                   IG = 0.07486\n",
      "colliery                   IG = 0.07481\n",
      "pursue                     IG = 0.07470\n",
      "custody                    IG = 0.07467\n",
      "application                IG = 0.07452\n",
      "austerity                  IG = 0.07444\n",
      "tories                     IG = 0.07436\n",
      "prosecution                IG = 0.07426\n",
      "iranian                    IG = 0.07413\n",
      "crimes                     IG = 0.07308\n",
      "ukraine                    IG = 0.07300\n",
      "prosperity                 IG = 0.07284\n",
      "hydrogen                   IG = 0.07272\n",
      "knees                      IG = 0.07262\n",
      "iconic                     IG = 0.07262\n",
      "civilian                   IG = 0.07239\n",
      "illegal                    IG = 0.07237\n",
      "regeneration               IG = 0.07231\n",
      "containing                 IG = 0.07227\n",
      "passage                    IG = 0.07216\n",
      "railways                   IG = 0.07212\n",
      "economies                  IG = 0.07209\n",
      "electric                   IG = 0.07190\n",
      "taxes                      IG = 0.07181\n",
      "mismanagement              IG = 0.07166\n",
      "departmental               IG = 0.07164\n",
      "gossip                     IG = 0.07161\n",
      "penny                      IG = 0.07143\n",
      "nutrient                   IG = 0.07142\n",
      "albans                     IG = 0.07142\n",
      "wealthiest                 IG = 0.07141\n",
      "rulings                    IG = 0.07139\n",
      "truss                      IG = 0.07125\n",
      "intended                   IG = 0.07107\n",
      "prosecute                  IG = 0.07097\n",
      "barnett                    IG = 0.07087\n",
      "gas                        IG = 0.07087\n",
      "targeting                  IG = 0.07065\n",
      "active                     IG = 0.07046\n",
      "perpetrated                IG = 0.07032\n",
      "january                    IG = 0.07026\n",
      "terrorism                  IG = 0.07019\n",
      "punishment                 IG = 0.07013\n",
      "shocks                     IG = 0.06992\n",
      "departments                IG = 0.06992\n",
      "disrupt                    IG = 0.06968\n",
      "\n",
      "ğŸ“Š ×©××¨×ª×™ ××ª ×ª×•×¦××•×ª ×”-IG ×œ××§×¡×œ ×‘: models/third_method_words_term_information_gain_clusters.xlsx\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e136f94e4097ad7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (IR Project)",
   "language": "python",
   "name": "ir_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
