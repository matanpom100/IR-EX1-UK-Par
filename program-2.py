#!/usr/bin/env python
# coding: utf-8

# 

# In[ ]:





# In[1]:


import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from tqdm import tqdm

def download_debate_files():
    """
    Downloads UK Parliament debate files from the specified start file.
    """
    # ×›×ª×•×‘×•×ª ×•×§×‘×¦×™× ×œ×¤×™ ×”×’×“×¨×•×ª ×”×ª×¨×’×™×œ
    base_url = "https://www.theyworkforyou.com/pwdata/scrapedxml/debates/"  # 
    start_file = "debates2023-06-28d.xml"  # 
    output_dir = "data.xml_files"

    # ×™×¦×™×¨×ª ×ª×™×§×™×™×ª ×¤×œ×˜ ×× ×”×™× ×œ× ×§×™×™××ª
    os.makedirs(output_dir, exist_ok=True)

    print(f"Connecting to {base_url} to get the file list...")

    try:
        # 1. ×§×‘×œ×ª ×¨×©×™××ª ×›×œ ×”×§×‘×¦×™× ××”×¢××•×“
        response = requests.get(base_url)
        response.raise_for_status()  # ×‘×“×™×§×” ×©×”×‘×§×©×” ×”×¦×œ×™×—×”
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 2. ×¡×™× ×•×Ÿ ×”×¨×©×™××” ×œ×§×‘×¦×™ XML ×‘×œ×‘×“
        all_xml_files = []
        for link in soup.find_all('a'):
            href = link.get('href')
            if href and href.endswith('.xml'):
                all_xml_files.append(href)
        
        print(f"Found {len(all_xml_files)} total XML files on the server.")

        # 3. ××™×ª×•×¨ ×§×•×‘×¥ ×”×”×ª×—×œ×” ×•×¡×™× ×•×Ÿ ×”×¨×©×™××” ×”×¡×•×¤×™×ª
        try:
            start_index = all_xml_files.index(start_file)
            files_to_download = all_xml_files[start_index:]
            print(f"Found start file. Preparing to download {len(files_to_download)} files...")
        except ValueError:
            print(f"Error: Could not find the starting file '{start_file}' in the list.")
            print("Please check the file name and try again.")
            return

        # 4. ×”×•×¨×“×ª ×”×§×‘×¦×™×
        # ×©×™××•×© ×‘-tqdm ×›×“×™ ×œ×”×¦×™×’ ××“ ×”×ª×§×“××•×ª
        for filename in tqdm(files_to_download, desc="Downloading files", unit="file"):
            file_url = urljoin(base_url, filename)
            local_path = os.path.join(output_dir, filename)

            # ×‘×“×™×§×” ×× ×”×§×•×‘×¥ ×›×‘×¨ ×§×™×™× ×›×“×™ ×œ×× ×•×¢ ×”×•×¨×“×” ×›×¤×•×œ×”
            if os.path.exists(local_path):
                continue
            
            try:
                # ×”×•×¨×“×ª ×”×§×•×‘×¥
                file_response = requests.get(file_url)
                file_response.raise_for_status()
                
                # ×©××™×¨×ª ×”×§×•×‘×¥
                with open(local_path, 'wb') as f:
                    f.write(file_response.content)
            
            except requests.exceptions.RequestException as e:
                print(f"Failed to download {filename}: {e}")

        print("\nDownload complete!")
        print(f"All files are saved in the '{output_dir}' directory.")

    except requests.exceptions.RequestException as e:
        print(f"Failed to access the website {base_url}: {e}")

if __name__ == "__main__":
    download_debate_files()


# In[3]:


import xml.etree.ElementTree as ET
import os
import re
from collections import defaultdict

# ×”×’×“×¨×•×ª × ×ª×™×‘×™×
XML_DIR = os.path.join('data', 'xml_files')
OUTPUT_DIR = os.path.join('data', 'combined_xml_files')
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- ×”×¤×•× ×§×¦×™×” is_redirect_file() ×”×•×¡×¨×” ---

def extract_text_from_xml(xml_file_path):
    """××—×–×™×¨ ××ª ×›×œ ×”×˜×§×¡×˜ ×”×’×•×œ××™ ××ª×•×š ×ª×’×™ <p> ×‘×œ×‘×“."""
    all_text = []
    
    try:
        tree = ET.parse(xml_file_path)
        root = tree.getroot()

        # --- ×©×™× ×•×™ ---
        # ×× ×—× ×• ×›×‘×¨ ×œ× ××¡× × ×™×. ×× ×—× ×• ×¤×©×•×˜ ××—×¤×©×™× ×ª×’×™ <p>.
        # ×× ×‘×§×•×‘×¥ ××™×Ÿ ×ª×’×™ <p> (×œ××©×œ, ×§×•×‘×¥ ×”×¤× ×™×” ×˜×”×•×¨),
        # ×”×œ×•×œ××” ×¤×©×•×˜ ×ª×“×œ×’, ×•×”×¤×•× ×§×¦×™×” ×ª×—×–×™×¨ ×˜×§×¡×˜ ×¨×™×§.
        # ×× ×™×© ×ª×’×™ <p> (×›××• ×‘×§×•×‘×¥ ×”××¢×•×¨×‘), ×”× ×™×—×•×œ×¦×•.
        # --- ×¡×•×£ ×©×™× ×•×™ ---

        for p_tag in root.findall('.//p'):
            if p_tag.text and p_tag.text.strip():
                all_text.append(p_tag.text.strip())

    except ET.ParseError as e:
        print(f"×©×’×™××ª × ×™×ª×•×— XML ×‘×§×•×‘×¥ {xml_file_path}: {e}")
        return ""
    except Exception as e:
        print(f"×©×’×™××” ×›×œ×œ×™×ª ×‘×§×•×‘×¥ {xml_file_path}: {e}")
        return ""
        
    return ' '.join(all_text)

# --- ×©××¨ ×”×œ×•×’×™×§×” ×–×”×” ×œ×—×œ×•×˜×™×Ÿ ---

combined_texts = defaultdict(list)
file_list = os.listdir(XML_DIR)

print(f"× ××¦××• {len(file_list)} ×§×‘×¦×™×. ××ª×—×™×œ×™× ×‘×—×™×œ×•×¥ ×•××™×—×•×“...")

for filename in file_list:
    if not filename.endswith('.xml'):
        continue
        
    file_path = os.path.join(XML_DIR, filename)
    
    match = re.search(r'debates(\d{4}-\d{2}-\d{2})[a-zA-Z]*d?\.xml$', filename)
    if not match:
        continue
        
    base_date = match.group(1)
    
    # ×”×¤×•× ×§×¦×™×” ×¢×›×©×™×• ××—×œ×¦×ª ×˜×§×¡×˜ ××›×œ ×§×•×‘×¥, ×‘×œ×™ ×§×©×¨ ×œ×”×¤× ×™×•×ª
    raw_text = extract_text_from_xml(file_path)
    
    if raw_text:
        combined_texts[base_date].append(raw_text)

print(f"×¡×™×•× ×—×™×œ×•×¥. × ××¦××• {len(combined_texts)} ×ª××¨×™×›×™× ×™×™×—×•×“×™×™×.")

# ×©××™×¨×ª ×”×§×‘×¦×™× ×”×××•×—×“×™×
for date, text_list in combined_texts.items():
    final_combined_text = ' '.join(text_list)
    
    # ×©× ×”×§×•×‘×¥ ×”×××•×—×“: debates_YYYY-MM-DD.txt
    output_filename = f"debates_{date}.txt"
    output_path = os.path.join(OUTPUT_DIR, output_filename)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(final_combined_text)

print(f"×”×˜×§×¡×˜×™× ×”×××•×—×“×™× (××‘×•×¡×¡×™ <p>) × ×©××¨×• ×‘×”×¦×œ×—×” ×‘×¡×¤×¨×™×™×” **{OUTPUT_DIR}**")


# In[7]:


import nltk
nltk.download('punkt')


# In[8]:


import nltk
nltk.download('punkt_tab')


# In[10]:


import spacy
import os

# ×”×’×“×¨×•×ª × ×ª×™×‘×™×
INPUT_DIR = os.path.join('data', 'combined_xml_files')
OUTPUT_DIR = os.path.join('data', 'tokenized_text_spacy') # ×©× ×—×“×©
os.makedirs(OUTPUT_DIR, exist_ok=True)

# 1. ×˜×¢×™× ×ª ××•×“×œ ×”×©×¤×” ×”×§×˜×Ÿ ×©×œ spaCy
# × ×˜×¢×Ÿ ××•×ª×• ×¤×¢× ××—×ª ××—×•×¥ ×œ×œ×•×œ××” ×œ×—×™×¡×›×•×Ÿ ×‘×–××Ÿ
print("×˜×•×¢×Ÿ ××ª ××•×“×œ ×”×©×¤×” ×©×œ spaCy...")
nlp = spacy.load("en_core_web_sm")
print("×”××•×“×œ × ×˜×¢×Ÿ.")

print(f"××ª×—×™×œ × ×™×§×•×™ ×¡×™×× ×™ ×¤×™×¡×•×§ (×¢× spaCy). ×§×‘×¦×™× ×™×™×©××¨×• ×‘- {OUTPUT_DIR}")

# ×§×¨×™××” ×©×œ ×›×œ ×”×§×‘×¦×™× ×”×××•×—×“×™×
for filename in os.listdir(INPUT_DIR):
    if filename.endswith('.txt'):
        input_path = os.path.join(INPUT_DIR, filename)
        
        with open(input_path, 'r', encoding='utf-8') as f:
            raw_text = f.read()
            
        # 2. ×¢×™×‘×•×“ ×”×˜×§×¡×˜ ×¢× spaCy
        # ×–×• ×”×“×¨×š ×©×‘×” spaCy ××‘×¦×¢ Tokenization
        doc = nlp(raw_text)
        
        # 3. ×—×™×œ×•×¥ ×”×˜×§×¡×˜ ×©×œ ×›×œ ×˜×•×§×Ÿ
        # spaCy ×©×•××¨ ×›×œ ×˜×•×§×Ÿ ×›××•×‘×™×™×§×˜, ×× ×—× ×• × ×™×§×— ×¨×§ ××ª ×”×˜×§×¡×˜ ×©×œ×•
        tokens = [token.text for token in doc]
        
        # 4. ×—×™×‘×•×¨ ×”×˜×•×§× ×™× ×‘×—×–×¨×” ×œ××—×¨×•×–×ª ×¢× ×¨×•×•×—×™×
        cleaned_text = ' '.join(tokens)
        
        # 5. ×©××™×¨×ª ×”×§×•×‘×¥ ×”× ×§×™
        output_path = os.path.join(OUTPUT_DIR, filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(cleaned_text)

print("×¡×™×•× × ×™×§×•×™ ×¡×™×× ×™ ×¤×™×¡×•×§ (spaCy).")


# In[ ]:


import os
import glob
import json
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from rank_bm25 import BM25Okapi
from collections import Counter
import nltk

nltk.download("punkt")
nltk.download("stopwords")

STOP_WORDS = set(stopwords.words("english"))

def load_texts(folder_path):
    docs, filenames = [], []
    for file_path in glob.glob(os.path.join(folder_path, "*.txt")):
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().lower()
            tokens = [w for w in word_tokenize(text) if w.isalpha() and w not in STOP_WORDS]
            docs.append(tokens)
            filenames.append(os.path.basename(file_path))
    return docs, filenames

def filter_rare_words(docs, min_freq=5):
    freq = Counter([word for doc in docs for word in doc])
    filtered_docs = [[w for w in doc if freq[w] >= min_freq] for doc in docs]
    return filtered_docs

def save_json_vectors(docs, filenames, output_dir):
    os.makedirs(output_dir, exist_ok=True)
    bm25 = BM25Okapi(docs)
    for doc_tokens, fname in zip(docs, filenames):
        vector = {}
        for term in bm25.idf.keys():
            f = doc_tokens.count(term)
            if f > 0:
                score = bm25.idf[term] * ((f * (bm25.k1 + 1)) / (f + bm25.k1 * (1 - bm25.b + bm25.b * len(doc_tokens) / bm25.avgdl)))
                vector[term] = round(score, 3)
        with open(os.path.join(output_dir, fname.replace(".txt", ".json")), "w", encoding="utf-8") as f:
            json.dump(vector, f, ensure_ascii=False, indent=2)

# === ×”×¤×¢×œ×ª ×”×ª×”×œ×™×š ===
base_data = "data"
base_model = "models"

# ×¢×™×‘×•×“ ×’×¨×¡×ª spaCy
spacy_docs, spacy_names = load_texts(os.path.join(base_data, "tokenized_text_spacy"))
spacy_docs = filter_rare_words(spacy_docs)
save_json_vectors(spacy_docs, spacy_names, os.path.join(base_model, "bm25_word_json_dict"))

# ×¢×™×‘×•×“ ×’×¨×¡×ª Lemmatized
lemm_docs, lemm_names = load_texts(os.path.join(base_data, "lemmatized_text"))
lemm_docs = filter_rare_words(lemm_docs)
save_json_vectors(lemm_docs, lemm_names, os.path.join(base_model, "bm25_lemm_json_dict"))

print("âœ… Done! All vectors saved per document.")


# In[4]:


import os
import glob
import json
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from rank_bm25 import BM25Okapi
from collections import Counter
import nltk

nltk.download("punkt", quiet=True)
nltk.download("stopwords", quiet=True)

STOP_WORDS = set(stopwords.words("english"))

def load_texts(folder_path):
    docs, filenames = [], []
    print(f"×˜×•×¢×Ÿ ×•×× ×§×” ×˜×§×¡×˜×™× ×: {folder_path}")
    for file_path in glob.glob(os.path.join(folder_path, "*.txt")):
        with open(file_path, "r", encoding="utf-8") as f:
            text = f.read().lower()
            tokens = [w for w in word_tokenize(text) if w.isalpha() and w not in STOP_WORDS]
            docs.append(tokens)
            filenames.append(os.path.basename(file_path))
    return docs, filenames

def filter_rare_words(docs, min_freq=5):
    print("××¡× ×Ÿ ××™×œ×™× × ×“×™×¨×•×ª...")
    freq = Counter([word for doc in docs for word in doc])
    # ×©×•××¨×™× ××ª ××•×¦×¨ ×”××™×œ×™× ×©××™× ×• × ×“×™×¨
    vocab = set(word for word, count in freq.items() if count >= min_freq)
    filtered_docs = [[w for w in doc if w in vocab] for doc in docs]
    print(f"×’×•×“×œ ××•×¦×¨ ×”××™×œ×™× ×”××§×•×¨×™: {len(freq)}, ××—×¨×™ ×¡×™× ×•×Ÿ: {len(vocab)}")
    return filtered_docs, vocab

def save_json_vectors_optimized(docs, filenames, output_dir, vocab):
    """
    ×’×¨×¡×” ××”×™×¨×”: ××—×©×‘×ª BM25 ×¨×§ ×¢×‘×•×¨ ×”××™×œ×™× ×©×‘×××ª ××•×¤×™×¢×•×ª ×‘×›×œ ××¡××š.
    """
    os.makedirs(output_dir, exist_ok=True)
    print(f"×‘×•× ×” ××•×“×œ BM25 Okapi ×¢×‘×•×¨ {output_dir}...")
    bm25 = BM25Okapi(docs)
    
    print("×™×•×¦×¨ ×•×©×•××¨ ×•×§×˜×•×¨×™× (×‘×©×™×˜×” ×”××”×™×¨×”)...")
    for doc_tokens, fname in zip(docs, filenames):
        vector = {}
        
        # --- 1. ×”××•×¤×˜×™××™×–×¦×™×” ---
        # ×¡×•×¤×¨×™× ××ª ×”××™×œ×™× ×¨×§ ×¤×¢× ××—×ª ×¢×‘×•×¨ ×”××¡××š ×”× ×•×›×—×™
        doc_freqs = Counter(doc_tokens)
        
        # --- 2. ×”××•×¤×˜×™××™×–×¦×™×” ---
        # ×¨×¦×™× *×¨×§* ×¢×œ ×”××™×œ×™× ×”×™×™×—×•×“×™×•×ª ×‘××¡××š ×–×” (×œ× ×¢×œ ×›×œ ×”××™×œ×•×Ÿ)
        for term in doc_freqs.keys():
            # (××™×Ÿ ×¦×•×¨×š ×‘-if f > 0 ×›×™ ×× ×—× ×• ×™×•×“×¢×™× ×©×”××™×œ×” ×§×™×™××ª)
            f = doc_freqs[term]
            
            # --- ×–×”×™×¨×•×ª: `rank_bm25` ×œ× ××—×–×™×¨ ×¦×™×•×Ÿ ×œ××™×œ×™× ×©×œ× ×‘-IDF ---
            # ××‘×œ ×× ×—× ×• ×¡×™× × ×• ××ª `docs` ×¢× `vocab` ××– ×–×” ×××•×¨ ×œ×”×™×•×ª ×‘×¡×“×¨
            if term not in bm25.idf:
                continue # ××™×œ×” ×–×• ×”×™×™×ª×” × ×“×™×¨×” ××“×™ ×•×”×•×¡×¨×”
                
            # ××•×ª×” × ×•×¡×—×” ×‘×“×™×•×§ ××”×§×•×“ ×©×œ×š
            score = bm25.idf[term] * ((f * (bm25.k1 + 1)) / (f + bm25.k1 * (1 - bm25.b + bm25.b * len(doc_tokens) / bm25.avgdl)))
            vector[term] = round(score, 3)
            
        # ×©××™×¨×ª ×§×•×‘×¥ ×”-JSON
        with open(os.path.join(output_dir, fname.replace(".txt", ".json")), "w", encoding="utf-8") as f:
            json.dump(vector, f, ensure_ascii=False, indent=2)

# === ×”×¤×¢×œ×ª ×”×ª×”×œ×™×š (×¢× ×”××•×¤×˜×™××™×–×¦×™×”) ===
base_data = "data"
base_model = "models"

# --- ×¢×™×‘×•×“ ×’×¨×¡×ª spaCy (Word) ---
spacy_docs_raw, spacy_names = load_texts(os.path.join(base_data, "tokenized_text_spacy"))
spacy_docs_filtered, spacy_vocab = filter_rare_words(spacy_docs_raw)
save_json_vectors_optimized(spacy_docs_filtered, spacy_names, 
                            os.path.join(base_model, "bm25_word_json_dict"), 
                            spacy_vocab)
print("--- ×¡×™×•× ×¢×™×‘×•×“ Word ---")


# --- ×¢×™×‘×•×“ ×’×¨×¡×ª Lemmatized (Lemm) ---
lemm_docs_raw, lemm_names = load_texts(os.path.join(base_data, "lemmatized_files"))
lemm_docs_filtered, lemm_vocab = filter_rare_words(lemm_docs_raw)
save_json_vectors_optimized(lemm_docs_filtered, lemm_names, 
                            os.path.join(base_model, "bm25_lemm_json_dict"), 
                            lemm_vocab)
print("--- ×¡×™×•× ×¢×™×‘×•×“ Lemm ---")

print("âœ… Done! All optimized vectors saved per document.")


# In[7]:


import os  # ğŸš€ ×”×•×¡×¤× ×• ××ª ×–×”
import glob
import json
import re
import sys
import subprocess
import importlib.util

# --- Auto-install gensim if not found ---
if importlib.util.find_spec("gensim") is None:
    print("Installing gensim ...")
    subprocess.check_call([sys.executable, "-m", "pip", "install", "gensim"])

# Now safe to import
from gensim.models import Word2Vec
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import numpy as np
import nltk

# --- Downloads for NLTK ---
nltk.download("punkt", quiet=True)
nltk.download("stopwords", quiet=True)
STOP_WORDS = set(stopwords.words("english"))

# --- Utility functions ---

def clean_text(text):
    """Remove punctuation, digits, quotes, etc."""
    text = re.sub(r"[^\w\s]", " ", text)  # remove punctuation
    text = re.sub(r"\d+", " ", text)      # remove numbers
    text = text.lower().strip()
    return text

def load_docs(folder, remove_stop=False):
    """Load all txt files and tokenize"""
    docs, names = [], []
    for path in glob.glob(os.path.join(folder, "*.txt")):
        with open(path, "r", encoding="utf-8") as f:
            text = clean_text(f.read())
            tokens = [w for w in word_tokenize(text) if w.isalpha()]
            if remove_stop:
                tokens = [w for w in tokens if w not in STOP_WORDS]
            docs.append(tokens)
            names.append(os.path.basename(path))
    return docs, names

def build_doc_vectors(docs, model):
    """Average all word vectors in each document"""
    vectors = []
    for tokens in docs:
        valid = [w for w in tokens if w in model.wv]
        if valid:
            vecs = np.array([model.wv[w] for w in valid])
            vectors.append(np.mean(vecs, axis=0))
        else:
            vectors.append(np.zeros(model.vector_size))
    return vectors

def save_vectors(vectors, names, out_dir):
    """Save each document vector as its own JSON"""
    os.makedirs(out_dir, exist_ok=True)
    for vec, name in zip(vectors, names):
        with open(os.path.join(out_dir, name.replace(".txt", ".json")), "w", encoding="utf-8") as f:
            json.dump({"vector": vec.tolist()}, f, ensure_ascii=False, indent=2)

# --- Main Configuration ---
base_data = "data"
base_model = "models"
VECTOR_SIZE = 300

# --- ğŸš€ ××•×¤×˜×™××™×–×¦×™×”: ×©×™××•×© ×‘×›×œ ×”×œ×™×‘×•×ª ×”×–××™× ×•×ª ---
CPU_CORES = os.cpu_count() or 1 # ( or 1 ×œ××§×¨×” ×©×”×¤×§×•×“×” × ×›×©×œ×ª)
print(f"Word2Vec | ××¤×¢×™×œ ××•×¤×˜×™××™×–×¦×™×”: ××©×ª××© ×‘-{CPU_CORES} ×œ×™×‘×•×ª CPU.")


# --- 1ï¸âƒ£ tokenized_text_spacy - with stopwords ---
print("\n××ª×—×™×œ ××•×“×œ 1/4: Word (×¢× stop-words)")
docs, names = load_docs(os.path.join(base_data, "tokenized_text_spacy"), remove_stop=False)
model = Word2Vec(sentences=docs, vector_size=VECTOR_SIZE, window=5, min_count=2, workers=CPU_CORES) # ğŸš€
save_vectors(build_doc_vectors(docs, model), names, os.path.join(base_model, "w2v_word_with_stop"))

# --- 2ï¸âƒ£ tokenized_text_spacy - no stopwords ---
print("\n××ª×—×™×œ ××•×“×œ 2/4: Word (×‘×œ×™ stop-words)")
docs_ns, names_ns = load_docs(os.path.join(base_data, "tokenized_text_spacy"), remove_stop=True)
model_ns = Word2Vec(sentences=docs_ns, vector_size=VECTOR_SIZE, window=5, min_count=2, workers=CPU_CORES) # ğŸš€
save_vectors(build_doc_vectors(docs_ns, model_ns), names_ns, os.path.join(base_model, "w2v_word_no_stop"))

# --- 3ï¸âƒ£ lemmatized_text - with stopwords ---
print("\n××ª×—×™×œ ××•×“×œ 3/4: Lemma (×¢× stop-words)")
lemm_docs, lemm_names = load_docs(os.path.join(base_data, "lemmatized_files"), remove_stop=False)
model_lemm = Word2Vec(sentences=lemm_docs, vector_size=VECTOR_SIZE, window=5, min_count=2, workers=CPU_CORES) # ğŸš€
save_vectors(build_doc_vectors(lemm_docs, model_lemm), lemm_names, os.path.join(base_model, "w2v_lemm_with_stop"))

# --- 4ï¸âƒ£ lemmatized_text - no stopwords ---
print("\n××ª×—×™×œ ××•×“×œ 4/4: Lemma (×‘×œ×™ stop-words)")
lemm_docs_ns, lemm_names_ns = load_docs(os.path.join(base_data, "lemmatized_files"), remove_stop=True)
model_lemm_ns = Word2Vec(sentences=lemm_docs_ns, vector_size=VECTOR_SIZE, window=5, min_count=2, workers=CPU_CORES) # ğŸš€
save_vectors(build_doc_vectors(lemm_docs_ns, model_lemm_ns), lemm_names_ns, os.path.join(base_model, "w2v_lemm_no_stop"))

print("\nâœ… Done! 4 folders with 300-dimensional Word2Vec document vectors created.")


# In[ ]:


import os
import glob
import json
import torch
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

print("--- ×”×ª×—×œ×ª ×¡×¢×™×£ ×’': SimCSE ---")

# --- 1. ×”×’×“×¨×ª × ×ª×™×‘×™× ---
INPUT_DIR = os.path.join('data', 'combined_xml_files') 
OUTPUT_DIR = os.path.join('models', 'simcse_origen')
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- 2. ×‘×“×™×§×ª ×”××¦×ª GPU (MPS) ×¢×‘×•×¨ M1/M2 ---
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("×–×™×”×•×™ M1/M2 GPU (MPS). ××¤×¢×™×œ ×”××¦×”.")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("×–×™×”×•×™ NVIDIA GPU (CUDA). ××¤×¢×™×œ ×”××¦×”.")
else:
    device = torch.device("cpu")
    print("×œ× ×–×•×”×ª×” ×”××¦×ª GPU. ××©×ª××© ×‘-CPU.")

# --- 3. ×˜×¢×™× ×ª ××•×“×œ SimCSE ---
model_name = 'princeton-nlp/unsup-simcse-bert-base-uncased'
print(f"×˜×•×¢×Ÿ ××ª ×”××•×“×œ {model_name}...")
model = SentenceTransformer(model_name, device=device)
print("×”××•×“×œ × ×˜×¢×Ÿ.")

# --- 4. ×˜×¢×™× ×ª ×”××¡××›×™× ---
all_texts = []
all_names = []
print(f"×˜×•×¢×Ÿ ××ª ×›×œ ×§×‘×¦×™ ×”××§×•×¨ ×-{INPUT_DIR}...")
for file_path in glob.glob(os.path.join(INPUT_DIR, "*.txt")):
    with open(file_path, "r", encoding="utf-8") as f:
        all_texts.append(f.read())
    all_names.append(os.path.basename(file_path))
print(f"× ××¦××• {len(all_texts)} ××¡××›×™×.")

# --- 5. ×™×¦×™×¨×ª ×•×§×˜×•×¨×™× (Embeddings) ---
print("××ª×—×™×œ ×‘×™×¦×™×¨×ª ×•×§×˜×•×¨×™ SimCSE...")
vectors = model.encode(
    all_texts, 
    show_progress_bar=True, 
    batch_size=32, # ××¤×©×¨ ×œ×”×’×“×™×œ ×× ×™×© ×œ×š ×”×¨×‘×” VRAM
    convert_to_numpy=True 
)
print("×™×¦×™×¨×ª ×”×•×§×˜×•×¨×™× ×”×•×©×œ××”.")

# --- 6. ×©××™×¨×ª ×”×•×§×˜×•×¨×™× ---
print(f"×©×•××¨ ××ª ×”×•×§×˜×•×¨×™× ×‘-{OUTPUT_DIR}...")
for vec, name in zip(vectors, all_names):
    output_path = os.path.join(OUTPUT_DIR, name.replace(".txt", ".json"))
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({"vector": vec.tolist()}, f, ensure_ascii=False, indent=2)

print("âœ… ×¡×™×•× ×¡×¢×™×£ ×’'! ×•×§×˜×•×¨×™ SimCSE × ×©××¨×•.")


# In[2]:


import os
import glob
import json
import torch
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

print("--- ×”×ª×—×œ×ª ×¡×¢×™×£ ×“': SBERT ---")

# --- 1. ×”×’×“×¨×ª × ×ª×™×‘×™× ---
# ×©×™××•×© ×‘××•×ª× ×§×‘×¦×™ ××§×•×¨ ×›××• SimCSE
INPUT_DIR = os.path.join('data', 'combined_xml_files') 
# ×ª×™×§×™×™×ª ×¤×œ×˜ ×œ×¤×™ ×©× ×”×§×‘×•×¦×” ×‘××˜×œ×”
OUTPUT_DIR = os.path.join('models', 'sbert_origen')
os.makedirs(OUTPUT_DIR, exist_ok=True)

# --- 2. ×‘×“×™×§×ª ×”××¦×ª GPU (MPS) ---
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("×–×™×”×•×™ M1/M2 GPU (MPS). ××¤×¢×™×œ ×”××¦×”.")
elif torch.cuda.is_available():
    device = torch.device("cuda")
    print("×–×™×”×•×™ NVIDIA GPU (CUDA). ××¤×¢×™×œ ×”××¦×”.")
else:
    device = torch.device("cpu")
    print("×œ× ×–×•×”×ª×” ×”××¦×ª GPU. ××©×ª××© ×‘-CPU.")

# --- 3. ×˜×¢×™× ×ª ××•×“×œ SBERT ---
# ×–×”×• ××•×“×œ SBERT ×¤×•×¤×•×œ×¨×™ ×•××•××œ×¥ ×œ×©×™××•×© ×›×œ×œ×™
model_name = 'all-mpnet-base-v2'
print(f"×˜×•×¢×Ÿ ××ª ×”××•×“×œ {model_name}...")
model = SentenceTransformer(model_name, device=device)
print("×”××•×“×œ × ×˜×¢×Ÿ.")

# --- 4. ×˜×¢×™× ×ª ×”××¡××›×™× ---
# ×”×§×•×“ ×–×”×” ×œ×¡×¢×™×£ ×”×§×•×“×
all_texts = []
all_names = []
print(f"×˜×•×¢×Ÿ ××ª ×›×œ ×§×‘×¦×™ ×”××§×•×¨ ×-{INPUT_DIR}...")
for file_path in glob.glob(os.path.join(INPUT_DIR, "*.txt")):
    with open(file_path, "r", encoding="utf-8") as f:
        all_texts.append(f.read())
    all_names.append(os.path.basename(file_path))
print(f"× ××¦××• {len(all_texts)} ××¡××›×™×.")

# --- 5. ×™×¦×™×¨×ª ×•×§×˜×•×¨×™× (Embeddings) ---
print("××ª×—×™×œ ×‘×™×¦×™×¨×ª ×•×§×˜×•×¨×™ SBERT...")
vectors = model.encode(
    all_texts, 
    show_progress_bar=True, 
    batch_size=32,
    convert_to_numpy=True 
)
print("×™×¦×™×¨×ª ×”×•×§×˜×•×¨×™× ×”×•×©×œ××”.")

# --- 6. ×©××™×¨×ª ×”×•×§×˜×•×¨×™× ---
print(f"×©×•××¨ ××ª ×”×•×§×˜×•×¨×™× ×‘-{OUTPUT_DIR}...")
for vec, name in zip(vectors, all_names):
    output_path = os.path.join(OUTPUT_DIR, name.replace(".txt", ".json"))
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({"vector": vec.tolist()}, f, ensure_ascii=False, indent=2)

print("âœ… ×¡×™×•× ×¡×¢×™×£ ×“'! ×•×§×˜×•×¨×™ SBERT × ×©××¨×•.")


# In[9]:


import os
from nltk.tokenize import word_tokenize

# ×”×’×“×¨×•×ª × ×ª×™×‘×™×
INPUT_DIR = os.path.join('data', 'combined_xml_files')
OUTPUT_DIR = os.path.join('data', 'tokenized_text_nltk') # ×©× ×—×“×© ×›×“×™ ×œ×”×‘×“×™×œ
os.makedirs(OUTPUT_DIR, exist_ok=True)

print(f"××ª×—×™×œ × ×™×§×•×™ ×¡×™×× ×™ ×¤×™×¡×•×§ (×¢× NLTK). ×§×‘×¦×™× ×™×™×©××¨×• ×‘- {OUTPUT_DIR}")

# ×§×¨×™××” ×©×œ ×›×œ ×”×§×‘×¦×™× ×”×××•×—×“×™×
for filename in os.listdir(INPUT_DIR):
    if filename.endswith('.txt'):
        input_path = os.path.join(INPUT_DIR, filename)
        
        with open(input_path, 'r', encoding='utf-8') as f:
            raw_text = f.read()
            
        # 1. ×©×™××•×© ×‘×˜×•×§× ×™×™×–×¨ ×”×—×›× ×©×œ NLTK
        # ×”×•× ×™×•×“×¢ ×œ×˜×¤×œ ×‘-don't, U.S.A., ×•×›×•' ×‘×¦×•×¨×” × ×›×•× ×”
        tokens = word_tokenize(raw_text)
        
        # 2. ×—×™×‘×•×¨ ×”×˜×•×§× ×™× ×‘×—×–×¨×” ×œ××—×¨×•×–×ª ×¢× ×¨×•×•×—×™×
        # ×”×ª×•×¦××” ×ª×”×™×” ×˜×§×¡×˜ ×©×‘×• ×¡×™×× ×™ ×”×¤×™×¡×•×§ ××•×¤×¨×“×™× ×›×”×œ×›×”
        cleaned_text = ' '.join(tokens)
        
        # 3. ×©××™×¨×ª ×”×§×•×‘×¥ ×”× ×§×™
        output_path = os.path.join(OUTPUT_DIR, filename)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(cleaned_text)

print("×¡×™×•× × ×™×§×•×™ ×¡×™×× ×™ ×¤×™×¡×•×§ (NLTK).")


# In[4]:


import os
import glob
import json
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_selection import mutual_info_classif, chi2
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans 
import numpy as np

print("--- ×”×ª×—×œ×ª ×¡×¢×™×£ ×”' (×’×™×©×ª K-Means ××©×•×œ×‘×ª) ---")

# --- 1. ×”×’×“×¨×ª × ×ª×™×‘×™× ---
# !!!
# !!! ×”×¨×¥ ××ª ×”×ª× ×¤×¢× ××—×ª ×¢× ×”× ×ª×™×‘ ×”×–×”:
TFIDF_DIR = os.path.join('models', 'bm25_word_json_dict')
# !!!
# !!! ×•××– ×©× ×” ×œ× ×ª×™×‘ ×”×‘× ×•×”×¨×¥ ×©×•×‘:
# TFIDF_DIR = os.path.join('models', 'bm25_lemm_json_dict')
# !!!
print(f"××¢×‘×“ ××ª ×”××˜×¨×™×¦×”: {TFIDF_DIR}")

# --- 2. ×˜×¢×™× ×ª ×”×•×§×˜×•×¨×™× (X) - ×¤×¢× ××—×ª ---
doc_vectors = [] # X
filenames = []   

print("×˜×•×¢×Ÿ ×§×‘×¦×™ JSON...")
for file_path in glob.glob(os.path.join(TFIDF_DIR, "*.json")):
    fname = os.path.basename(file_path)
    with open(file_path, 'r', encoding='utf-8') as f:
        doc_vectors.append(json.load(f))
    filenames.append(fname)

print(f"× ××¦××• {len(doc_vectors)} ××¡××›×™×.")

# --- 3. ×‘× ×™×™×ª ×”××˜×¨×™×¦×” (X) - ×¤×¢× ××—×ª ---
print("×××™×¨ ××ª ×¨×©×™××ª ×”-dicts ×œ××˜×¨×™×¦×ª ×¤×™×¦'×¨×™× (X)...")
vectorizer = DictVectorizer(sparse=True)
X_sparse = vectorizer.fit_transform(doc_vectors)
feature_names = vectorizer.get_feature_names_out()

print(f"×”××˜×¨×™×¦×” × ×•×¦×¨×” ×‘×’×•×“×œ: {X_sparse.shape} (××¡××›×™×, ×××¤×™×™× ×™×)")

# --- 4. ğŸ’¡ ×œ×•×œ××” ×¢×œ 3 ×¢×¨×›×™ K ×©×•× ×™× ---
# ×©× ×” ××ª ×”×¨×©×™××” ×”×–×• ×›×¨×¦×•× ×š
K_VALUES = [5, 10, 15] 

for k in K_VALUES:
    print("\n" + "="*60)
    print(f"--- ××ª×—×™×œ × ×™×ª×•×— ×¢×‘×•×¨ K = {k} ---")
    print("="*60)

    # --- 4a. ×™×¦×™×¨×ª ×ª×•×•×™×•×ª (y) ×‘×××¦×¢×•×ª K-Means ---
    print(f"××‘×¦×¢ ×§×™×‘×•×¥ (clustering) ×œ- {k} ×§×‘×•×¦×•×ª...")
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    y = kmeans.fit_predict(X_sparse) # ğŸš€ ×–×•×”×™ ×”-y ×”×—×“×©×” ×©×œ× ×•!
    
    print(f"×”×ª×¤×œ×’×•×ª ×”××¡××›×™× ×‘×§×‘×•×¦×•×ª (K={k}):")
    print(np.bincount(y))

    # --- 5. ×—×™×©×•×‘ ×”××“×“×™× (×¢× ×”×ª×•×•×™×•×ª ×”×—×“×©×•×ª ×-K-Means) ---

    # ××“×“ 1: Information Gain
    print(f"[K={k}] ××—×©×‘ Information Gain...")
    print(f" [K={k}] (×××™×¨ ×œ××˜×¨×™×¦×” ×¦×¤×•×¤×”)...")
    X_dense = X_sparse.toarray()
    ig_scores = mutual_info_classif(X_dense, y, discrete_features=False)
    ig_results = pd.DataFrame({'feature': feature_names, 'info_gain': ig_scores})
    ig_results = ig_results.sort_values(by='info_gain', ascending=False)

    # ××“×“ 2: Chi-squared
    print(f"[K={k}] ××—×©×‘ Chi-squared...")
    chi2_scores, p_values = chi2(X_sparse, y)
    chi2_results = pd.DataFrame({'feature': feature_names, 'chi2_score': chi2_scores, 'p_value': p_values})
    chi2_results = chi2_results.sort_values(by='chi2_score', ascending=False)

    # ××“×“ 3: Gini Impurity
    print(f"[K={k}] ××—×©×‘ Gini Impurity...")
    clf = DecisionTreeClassifier(random_state=42)
    clf.fit(X_sparse, y)
    gini_scores = clf.feature_importances_
    gini_results = pd.DataFrame({'feature': feature_names, 'gini_importance': gini_scores})
    gini_results = gini_results.sort_values(by='gini_importance', ascending=False)

    print(f"[K={k}] ×”×—×™×©×•×‘×™× ×”×¡×ª×™×™××•.")

    # --- 6. ×©××™×¨×ª ×”×ª×•×¦××•×ª ×œ××§×¡×œ ---
    excel_filename = f"feature_analysis_KMeans_k={k}_{os.path.basename(TFIDF_DIR)}.xlsx"
    print(f"×©×•××¨ ×ª×•×¦××•×ª ×œ×§×•×‘×¥: {excel_filename}")

    with pd.ExcelWriter(excel_filename) as writer:
        ig_results.to_excel(writer, sheet_name='Information Gain', index=False)
        chi2_results.to_excel(writer, sheet_name='Chi-squared', index=False)
        gini_results.to_excel(writer, sheet_name='Gini Importance', index=False)
    
    print(f"--- Top 10 ×ª×•×¦××•×ª ×¢×‘×•×¨ K={k} (×œ×”×©×•×•××” ××”×™×¨×”) ---")
    print(f"\n--- Top 10 Info Gain (K={k}) ---")
    print(ig_results.head(10))
    print(f"\n--- Top 10 Chi-squared (K={k}) ---")
    print(chi2_results.head(10))
    print(f"\n--- Top 10 Gini Importance (K={k}) ---")
    print(gini_results.head(10))

print("\n\nâœ…âœ…âœ… ×¡×™×•×! ×›×œ × ×™×ª×•×—×™ ×”-K-Means ×”×•×©×œ××•. âœ…âœ…âœ…")


# In[11]:


import os
import glob
import json
import pandas as pd
from sklearn.feature_extraction import DictVectorizer
from sklearn.feature_selection import mutual_info_classif, chi2
from sklearn.tree import DecisionTreeClassifier
from sklearn.cluster import KMeans 
import numpy as np
import matplotlib.pyplot as plt

print("--- ×”×ª×—×œ×ª ×¡×¢×™×£ ×”' (×’×™×©×ª K-Means, ×§×•×‘×¥ ×××•×—×“) ---")

# --- 1. ×”×’×“×¨×ª × ×ª×™×‘×™× ---
# !!!
# !!! ×”×¨×¥ ××ª ×”×ª× ×¤×¢× ××—×ª ×¢× ×”× ×ª×™×‘ ×”×–×”:
TFIDF_DIR = os.path.join('models', 'bm25_word_json_dict')
# !!!
# !!! ×•××– ×©× ×” ×œ× ×ª×™×‘ ×”×‘× ×•×”×¨×¥ ×©×•×‘:
# TFIDF_DIR = os.path.join('models', 'bm25_lemm_json_dict')
# !!!
print(f"××¢×‘×“ ××ª ×”××˜×¨×™×¦×”: {TFIDF_DIR}")

# --- 2. ×˜×¢×™× ×ª ×”×•×§×˜×•×¨×™× (X) - ×¤×¢× ××—×ª ---
doc_vectors = [] # X
filenames = []   

print("×˜×•×¢×Ÿ ×§×‘×¦×™ JSON...")
for file_path in glob.glob(os.path.join(TFIDF_DIR, "*.json")):
    fname = os.path.basename(file_path)
    with open(file_path, 'r', encoding='utf-8') as f:
        doc_vectors.append(json.load(f))
    filenames.append(fname)
print(f"× ××¦××• {len(doc_vectors)} ××¡××›×™×.")

# --- 3. ×‘× ×™×™×ª ×”××˜×¨×™×¦×” (X) - ×¤×¢× ××—×ª ---
print("×××™×¨ ××ª ×¨×©×™××ª ×”-dicts ×œ××˜×¨×™×¦×ª ×¤×™×¦'×¨×™× (X)...")
vectorizer = DictVectorizer(sparse=True)
X_sparse = vectorizer.fit_transform(doc_vectors)
feature_names = vectorizer.get_feature_names_out()
print(f"×”××˜×¨×™×¦×” × ×•×¦×¨×” ×‘×’×•×“×œ: {X_sparse.shape} (××¡××›×™×, ×××¤×™×™× ×™×)")

# --- 4. ğŸ’¡ ×”×›× ×ª ×”×˜×‘×œ××•×ª ×”×××•×—×“×•×ª ---
# × ×™×¦×•×¨ DataFrames ×¨×™×§×™× ×¢× ×”××™×œ×™× ×›×©×•×¨×•×ª (××™× ×“×§×¡)
ig_results_all = pd.DataFrame(index=feature_names)
chi2_results_all = pd.DataFrame(index=feature_names)
gini_results_all = pd.DataFrame(index=feature_names)

K_VALUES = [5, 10, 15, 20, 30, 40, 50, 75, 100, 150, 200] 
max_ig_scores = []
max_chi2_scores = []

# --- 5. ğŸ’¡ ×œ×•×œ××” ×¢×œ 3 ×¢×¨×›×™ K ×©×•× ×™× ---
for k in K_VALUES:
    print("\n" + "="*60)
    print(f"--- ××ª×—×™×œ × ×™×ª×•×— ×¢×‘×•×¨ K = {k} ---")
    print("="*60)

    # --- 5a. ×™×¦×™×¨×ª ×ª×•×•×™×•×ª (y) ×‘×××¦×¢×•×ª K-Means ---
    print(f"××‘×¦×¢ ×§×™×‘×•×¥ (clustering) ×œ- {k} ×§×‘×•×¦×•×ª...")
    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')
    y = kmeans.fit_predict(X_sparse) 
    print(f"×”×ª×¤×œ×’×•×ª ×”××¡××›×™× ×‘×§×‘×•×¦×•×ª (K={k}): {np.bincount(y)}")

    # --- 5b. ×—×™×©×•×‘ ×”××“×“×™× ---

    # ××“×“ 1: Information Gain
    print(f"[K={k}] ××—×©×‘ Information Gain...")
    X_dense = X_sparse.toarray()
    ig_scores = mutual_info_classif(X_dense, y, discrete_features=False)
    max_ig_scores.append(np.max(ig_scores))
    # ğŸ’¡ ×”×•×¡×¤×ª ×”×ª×•×¦××•×ª ×›×¢××•×“×” ×—×“×©×”
    ig_results_all[f'info_gain_k={k}'] = ig_scores

    # ××“×“ 2: Chi-squared
    print(f"[K={k}] ××—×©×‘ Chi-squared...")
    chi2_scores, p_values = chi2(X_sparse, y)
    max_chi2_scores.append(np.max(chi2_scores))
    # ğŸ’¡ ×”×•×¡×¤×ª ×”×ª×•×¦××•×ª ×›×¢××•×“×•×ª ×—×“×©×•×ª
    chi2_results_all[f'chi2_score_k={k}'] = chi2_scores
    chi2_results_all[f'p_value_k={k}'] = p_values

    # ××“×“ 3: Gini Impurity
    print(f"[K={k}] ××—×©×‘ Gini Impurity...")
    clf = DecisionTreeClassifier(random_state=42)
    clf.fit(X_sparse, y)
    gini_scores = clf.feature_importances_
    # ğŸ’¡ ×”×•×¡×¤×ª ×”×ª×•×¦××•×ª ×›×¢××•×“×” ×—×“×©×”
    gini_results_all[f'gini_importance_k={k}'] = gini_scores

    print(f"[K={k}] ×”×—×™×©×•×‘×™× ×”×¡×ª×™×™××•.")

    # --- ×”×“×¤×¡×ª Top 10 ×–×× ×™×ª ---
    print(f"\n--- Top 10 Info Gain (K={k}) ---")
    print(ig_results_all[[f'info_gain_k={k}']].sort_values(by=f'info_gain_k={k}', ascending=False).head(10))

print("\n\nâœ…âœ…âœ… ×›×œ ×—×™×©×•×‘×™ ×”-K-Means ×”×•×©×œ××•. âœ…âœ…âœ…")

# --- 6. ×©××™×¨×ª ×”×§×•×‘×¥ ×”×××•×—×“ ---
excel_filename = f"feature_analysis_KMeans_CONSOLIDATED_{os.path.basename(TFIDF_DIR)}.xlsx"
print(f"×©×•××¨ ××ª ×›×œ ×”×ª×•×¦××•×ª ×œ×§×•×‘×¥ ××§×¡×œ ××—×“: {excel_filename}")

# × ××™×™×Ÿ ×›×œ ×˜×‘×œ×” ×œ×¤×™ ×”×ª×•×¦××•×ª ×©×œ K=10 (××• K ×”×¨××©×•×Ÿ ×‘×¨×©×™××”)
k_to_sort_by = K_VALUES[1] if len(K_VALUES) > 1 else K_VALUES[0]

with pd.ExcelWriter(excel_filename) as writer:
    ig_results_all.sort_values(by=f'info_gain_k={k_to_sort_by}', ascending=False).to_excel(writer, sheet_name='Information Gain')
    chi2_results_all.sort_values(by=f'chi2_score_k={k_to_sort_by}', ascending=False).to_excel(writer, sheet_name='Chi-squared')
    gini_results_all.sort_values(by=f'gini_importance_k={k_to_sort_by}', ascending=False).to_excel(writer, sheet_name='Gini Importance')

print("âœ… ×¡×™×•×! ×”×§×•×‘×¥ ×”×××•×—×“ × ×©××¨ ×‘×”×¦×œ×—×”.")

# --- 7. ×™×¦×™×¨×ª ×’×¨×¤×™× ---
plt.figure(figsize=(12, 6))
plt.plot(K_VALUES, max_ig_scores, marker='o')
plt.title('Max Information Gain vs. K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Max Information Gain')
plt.grid(True)
plt.savefig('info_gain_vs_k.png')
plt.show()

plt.figure(figsize=(12, 6))
plt.plot(K_VALUES, max_chi2_scores, marker='o')
plt.title('Max Chi-squared vs. K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Max Chi-squared Score')
plt.grid(True)
plt.savefig('chi2_vs_k.png')
plt.show()

print("âœ… ×’×¨×¤×™× × ×©××¨×• ×‘×”×¦×œ×—×”.")



# In[11]:





# In[ ]:




